{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperTokenizerFast, WhisperFeatureExtractor,get_scheduler,WhisperProcessor\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "import datasets\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer, EnglishTextNormalizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25d170ac4214e54b5a191b9760ad46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ce28e974494fe391c82fd9db1ffbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54e64fcf6084d46836519c8e0619775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds1 = load_dataset('facebook/multilingual_librispeech',\n",
    "                    'german',\n",
    "                    cache_dir=\"/media/hdd/.cache/huggingface\",\n",
    "            )\n",
    "\n",
    "ds2 = load_dataset('mozilla-foundation/common_voice_16_0',\n",
    "                    'de',\n",
    "                    cache_dir=\"/media/hdd_old/.cache/huggingface\",\n",
    "                )\n",
    "\n",
    "ds3 = load_dataset(\n",
    "            'facebook/voxpopuli',\n",
    "            'de',\n",
    "            cache_dir=\"/media/hdd_old/.cache/huggingface\",\n",
    "            )\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"distil-whisper/distil-medium.en\")\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(\"distil-whisper/distil-medium.en\")\n",
    "\n",
    "def rename_columns(ds, column_nammes):\n",
    "    ds = ds.cast_column(\"audio\", datasets.features.Audio(16000))\n",
    "\n",
    "    ds = ds.rename_column(column_nammes, \"text\")\n",
    "    \n",
    "    dataset_features = ds['train'].features.keys()\n",
    "    columns_to_keep = {\"audio\", \"text\"}\n",
    "    ds = ds.remove_columns(set(dataset_features - columns_to_keep))\n",
    "    \n",
    "    return ds\n",
    "    \n",
    "ds1 = rename_columns(ds1, \"transcript\")\n",
    "ds2 = rename_columns(ds2, \"sentence\")\n",
    "ds3 = rename_columns(ds3, \"raw_text\")\n",
    "\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = concatenate_datasets([ds1['train'], ds2['train'], ds2['validation']])\n",
    "dataset['ID_eval'] = concatenate_datasets([ds1['test'], ds2['test']])\n",
    "dataset['OOD_eval'] = concatenate_datasets([ds3['validation'], ds3['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'] = dataset['train'].shuffle(seed=41).take(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Function {func.__name__} took {end_time - start_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['audio', 'text']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets_train_features = list(dataset[\"train\"].features.keys())\n",
    "raw_datasets_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def prepare_train_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = [sample[\"array\"] for sample in batch[\"audio\"]]\n",
    "    inputs = feature_extractor(audio, sampling_rate=16000, device='cuda')\n",
    "    batch[\"input_features\"] = inputs.input_features\n",
    "    batch[\"input_length\"] = [len(sample) for sample in audio]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[300, 800, 100, 500]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "list = [10, 100, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bd2a323cce43fd90e9e8a78ae5b225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function prepare_train_dataset took 5.2748 seconds\n",
      "Function prepare_train_dataset took 5.0388 seconds\n",
      "Function prepare_train_dataset took 5.1649 seconds\n",
      "Function prepare_train_dataset took 1.7884 seconds\n",
      "batch_size: 300, time: 117.6985 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fa53a4f7c64994a06e7a9af1d75f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function prepare_train_dataset took 14.6061 seconds\n",
      "Function prepare_train_dataset took 3.4520 seconds\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/hdd/.cache/huggingface/facebook___multilingual_librispeech/german/0.0.0/2e83e61823b4c47dcbcb1980bb88601274127609/tmp7wp0rd39'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/shutil.py:886\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 886\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/hdd/.cache/huggingface/facebook___multilingual_librispeech/german/0.0.0/2e83e61823b4c47dcbcb1980bb88601274127609/tmp7wp0rd39' -> '/media/hdd/.cache/huggingface/facebook___multilingual_librispeech/german/0.0.0/2e83e61823b4c47dcbcb1980bb88601274127609/cache-d4c5902c7013d02e.arrow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m      3\u001b[0m     tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare_train_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m tic\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3167\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3162\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3163\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3164\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3165\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3166\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3167\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3168\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3603\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_data \u001b[38;5;129;01mand\u001b[39;00m tmp_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3602\u001b[0m     tmp_file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 3603\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3604\u001b[0m     umask \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mumask(\u001b[38;5;241m0o666\u001b[39m)\n\u001b[1;32m   3605\u001b[0m     os\u001b[38;5;241m.\u001b[39mumask(umask)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/shutil.py:906\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    904\u001b[0m         rmtree(src)\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 906\u001b[0m         \u001b[43mcopy_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m         os\u001b[38;5;241m.\u001b[39munlink(src)\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m real_dst\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/shutil.py:475\u001b[0m, in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    473\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m copystat(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/shutil.py:260\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    258\u001b[0m     os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mreadlink(src), dst)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/hdd/.cache/huggingface/facebook___multilingual_librispeech/german/0.0.0/2e83e61823b4c47dcbcb1980bb88601274127609/tmp7wp0rd39'"
     ]
    }
   ],
   "source": [
    "for batch_size in list:\n",
    "    \n",
    "    tic = time.time()\n",
    "    dataset['train'].map(prepare_train_dataset, batched=True, batch_size=batch_size)\n",
    "    toc = time.time() - tic\n",
    "    \n",
    "    print(f\"batch_size: {batch_size}, time: {toc:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333333'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"3\" * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'30303030303030303030'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'30' * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"3\" * 10 > '30' * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf7523f2fcd404c9ed9bda259e787ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0029 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0053 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0030 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0030 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0030 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0031 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0031 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0052 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0057 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0055 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0067 seconds\n",
      "Function prepare_train_dataset took 0.0065 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0060 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0053 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0029 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0054 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0061 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0053 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0054 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0058 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0056 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0053 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0061 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0052 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0054 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0061 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0052 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0031 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0057 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0053 seconds\n",
      "Function prepare_train_dataset took 0.0054 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0030 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0072 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0052 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset['train'] = dataset['train'].map(prepare_train_dataset, batched=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17.19,\n",
       " 17.62,\n",
       " 10.991125,\n",
       " 18.8,\n",
       " 16.15,\n",
       " 15.87,\n",
       " 18.37,\n",
       " 14.54,\n",
       " 18.37,\n",
       " 18.95,\n",
       " 15.21,\n",
       " 17.57,\n",
       " 14.49,\n",
       " 16.92,\n",
       " 13.85,\n",
       " 11.12,\n",
       " 12.98,\n",
       " 11.38,\n",
       " 15.19,\n",
       " 12.74,\n",
       " 12.2,\n",
       " 13.63,\n",
       " 16.22,\n",
       " 18.06,\n",
       " 14.52,\n",
       " 17.82,\n",
       " 13.53,\n",
       " 11.66,\n",
       " 14.5,\n",
       " 14.67,\n",
       " 12.53,\n",
       " 13.24,\n",
       " 16.36,\n",
       " 15.68,\n",
       " 13.82,\n",
       " 12.54,\n",
       " 10.61,\n",
       " 17.68,\n",
       " 10.14,\n",
       " 16.68,\n",
       " 10.5,\n",
       " 12.3,\n",
       " 10.27,\n",
       " 13.81,\n",
       " 12.13,\n",
       " 10.29,\n",
       " 12.56,\n",
       " 16.62,\n",
       " 20.0,\n",
       " 12.05,\n",
       " 13.02,\n",
       " 14.53,\n",
       " 15.95,\n",
       " 11.4,\n",
       " 16.72,\n",
       " 14.62,\n",
       " 17.03,\n",
       " 12.86,\n",
       " 12.93,\n",
       " 19.98,\n",
       " 17.16,\n",
       " 14.61,\n",
       " 16.54,\n",
       " 16.91,\n",
       " 16.92,\n",
       " 19.2,\n",
       " 10.75,\n",
       " 14.71,\n",
       " 13.33,\n",
       " 17.99,\n",
       " 10.63,\n",
       " 18.62,\n",
       " 12.17,\n",
       " 10.23,\n",
       " 18.46,\n",
       " 10.72,\n",
       " 10.94,\n",
       " 14.29,\n",
       " 13.29,\n",
       " 11.62,\n",
       " 17.53,\n",
       " 10.72,\n",
       " 13.64,\n",
       " 14.96,\n",
       " 11.89,\n",
       " 17.71,\n",
       " 11.8,\n",
       " 14.87,\n",
       " 16.49,\n",
       " 12.36,\n",
       " 11.12,\n",
       " 14.54,\n",
       " 14.94,\n",
       " 13.21,\n",
       " 15.64,\n",
       " 14.11,\n",
       " 12.96,\n",
       " 17.94,\n",
       " 11.17,\n",
       " 19.72,\n",
       " 13.83,\n",
       " 10.52,\n",
       " 16.61,\n",
       " 12.58,\n",
       " 12.21,\n",
       " 16.04,\n",
       " 11.41,\n",
       " 20.0,\n",
       " 12.84,\n",
       " 11.73,\n",
       " 17.89,\n",
       " 11.01,\n",
       " 10.04,\n",
       " 11.48,\n",
       " 14.68,\n",
       " 11.57,\n",
       " 18.5,\n",
       " 13.78,\n",
       " 12.77,\n",
       " 11.63,\n",
       " 17.86,\n",
       " 15.18,\n",
       " 12.55,\n",
       " 10.14,\n",
       " 18.9,\n",
       " 10.31,\n",
       " 13.75,\n",
       " 10.0,\n",
       " 15.04,\n",
       " 12.05,\n",
       " 18.47,\n",
       " 10.57,\n",
       " 18.46,\n",
       " 13.12,\n",
       " 14.76,\n",
       " 14.61,\n",
       " 15.7,\n",
       " 14.45,\n",
       " 13.48,\n",
       " 17.95,\n",
       " 13.53,\n",
       " 13.35,\n",
       " 13.29,\n",
       " 16.16,\n",
       " 11.68,\n",
       " 10.44,\n",
       " 13.99,\n",
       " 11.3,\n",
       " 10.88,\n",
       " 16.33,\n",
       " 17.18,\n",
       " 19.18,\n",
       " 14.08,\n",
       " 11.6,\n",
       " 14.08,\n",
       " 11.11,\n",
       " 12.87,\n",
       " 14.83,\n",
       " 13.68,\n",
       " 12.98,\n",
       " 10.69,\n",
       " 14.48,\n",
       " 13.71,\n",
       " 11.17,\n",
       " 10.9,\n",
       " 12.03,\n",
       " 17.32,\n",
       " 14.25,\n",
       " 19.61,\n",
       " 10.27,\n",
       " 17.78,\n",
       " 13.65,\n",
       " 11.96,\n",
       " 14.2,\n",
       " 13.82,\n",
       " 13.7,\n",
       " 14.95,\n",
       " 15.01,\n",
       " 12.17,\n",
       " 11.26,\n",
       " 11.87,\n",
       " 15.17,\n",
       " 11.25,\n",
       " 15.27,\n",
       " 10.74,\n",
       " 13.84,\n",
       " 17.29,\n",
       " 11.08,\n",
       " 17.61,\n",
       " 18.16,\n",
       " 18.99,\n",
       " 14.08,\n",
       " 18.93,\n",
       " 15.8,\n",
       " 17.19,\n",
       " 15.38,\n",
       " 15.24,\n",
       " 15.26,\n",
       " 12.28,\n",
       " 14.55,\n",
       " 13.6,\n",
       " 14.41,\n",
       " 12.4,\n",
       " 10.03,\n",
       " 12.73,\n",
       " 11.28,\n",
       " 18.57,\n",
       " 17.52,\n",
       " 11.45,\n",
       " 12.12,\n",
       " 10.62,\n",
       " 17.6,\n",
       " 10.07,\n",
       " 15.56,\n",
       " 17.77,\n",
       " 11.97,\n",
       " 17.06,\n",
       " 13.63,\n",
       " 11.34,\n",
       " 10.22,\n",
       " 16.38,\n",
       " 11.83,\n",
       " 17.53,\n",
       " 19.11,\n",
       " 16.76,\n",
       " 10.49,\n",
       " 17.17,\n",
       " 16.61,\n",
       " 14.69,\n",
       " 15.64,\n",
       " 19.02,\n",
       " 19.45,\n",
       " 11.11,\n",
       " 18.73,\n",
       " 12.87,\n",
       " 17.56,\n",
       " 19.44,\n",
       " 12.43,\n",
       " 18.81,\n",
       " 18.65,\n",
       " 12.5,\n",
       " 18.55,\n",
       " 11.28,\n",
       " 12.99,\n",
       " 13.0,\n",
       " 14.75,\n",
       " 18.28,\n",
       " 11.1,\n",
       " 18.17,\n",
       " 18.4,\n",
       " 15.61,\n",
       " 15.01,\n",
       " 15.18,\n",
       " 18.29,\n",
       " 16.8,\n",
       " 13.17,\n",
       " 17.91,\n",
       " 19.35,\n",
       " 14.41,\n",
       " 19.04,\n",
       " 13.86,\n",
       " 13.67,\n",
       " 13.92,\n",
       " 11.47,\n",
       " 19.52,\n",
       " 11.24,\n",
       " 17.28,\n",
       " 14.84,\n",
       " 17.51,\n",
       " 16.84,\n",
       " 12.98,\n",
       " 10.1,\n",
       " 16.66,\n",
       " 15.22,\n",
       " 19.48,\n",
       " 16.8,\n",
       " 13.45,\n",
       " 12.43,\n",
       " 14.28,\n",
       " 16.27,\n",
       " 13.2,\n",
       " 16.92,\n",
       " 10.95,\n",
       " 16.41,\n",
       " 12.82,\n",
       " 13.15,\n",
       " 14.44,\n",
       " 17.0,\n",
       " 14.73,\n",
       " 15.64,\n",
       " 15.16,\n",
       " 12.42,\n",
       " 12.51,\n",
       " 14.97,\n",
       " 10.26,\n",
       " 13.36,\n",
       " 12.17,\n",
       " 15.76,\n",
       " 13.32,\n",
       " 11.75,\n",
       " 10.56,\n",
       " 16.43,\n",
       " 13.33,\n",
       " 10.85,\n",
       " 10.65,\n",
       " 17.6,\n",
       " 12.56,\n",
       " 14.55,\n",
       " 14.23,\n",
       " 19.32,\n",
       " 12.77,\n",
       " 15.33,\n",
       " 13.92,\n",
       " 18.16,\n",
       " 12.18,\n",
       " 13.13,\n",
       " 11.69,\n",
       " 15.84,\n",
       " 11.9,\n",
       " 14.15,\n",
       " 14.01,\n",
       " 14.36,\n",
       " 15.67,\n",
       " 10.48,\n",
       " 19.48,\n",
       " 13.97,\n",
       " 14.94,\n",
       " 14.75,\n",
       " 16.07,\n",
       " 14.51,\n",
       " 14.47,\n",
       " 13.11,\n",
       " 15.61,\n",
       " 10.74,\n",
       " 19.45,\n",
       " 15.91,\n",
       " 14.26,\n",
       " 13.33,\n",
       " 11.58,\n",
       " 16.29,\n",
       " 16.33,\n",
       " 17.01,\n",
       " 14.29,\n",
       " 13.31,\n",
       " 11.27,\n",
       " 10.91,\n",
       " 19.23,\n",
       " 17.64,\n",
       " 10.69,\n",
       " 19.27,\n",
       " 15.26,\n",
       " 16.37,\n",
       " 18.56,\n",
       " 17.12,\n",
       " 11.22,\n",
       " 13.41,\n",
       " 16.75,\n",
       " 14.16,\n",
       " 14.67,\n",
       " 15.36,\n",
       " 16.231125,\n",
       " 12.7,\n",
       " 12.43,\n",
       " 11.07,\n",
       " 14.35,\n",
       " 19.37,\n",
       " 10.8,\n",
       " 16.94,\n",
       " 12.64,\n",
       " 16.87,\n",
       " 13.34,\n",
       " 16.83,\n",
       " 19.91,\n",
       " 15.03,\n",
       " 11.07,\n",
       " 12.08,\n",
       " 14.83,\n",
       " 18.01,\n",
       " 14.97,\n",
       " 11.6,\n",
       " 18.71,\n",
       " 13.68,\n",
       " 12.6,\n",
       " 12.49,\n",
       " 14.47,\n",
       " 12.01,\n",
       " 19.36,\n",
       " 17.38,\n",
       " 17.93,\n",
       " 17.26,\n",
       " 14.7,\n",
       " 12.02,\n",
       " 19.81,\n",
       " 13.71,\n",
       " 19.11,\n",
       " 13.76,\n",
       " 19.27,\n",
       " 12.31,\n",
       " 17.21,\n",
       " 11.37,\n",
       " 19.75,\n",
       " 15.22,\n",
       " 10.21,\n",
       " 10.46,\n",
       " 18.37,\n",
       " 12.02,\n",
       " 13.23,\n",
       " 17.54,\n",
       " 12.88,\n",
       " 15.38,\n",
       " 14.4,\n",
       " 18.82,\n",
       " 10.25,\n",
       " 13.7,\n",
       " 15.42,\n",
       " 13.91,\n",
       " 11.17,\n",
       " 11.05,\n",
       " 14.72,\n",
       " 13.06,\n",
       " 11.23,\n",
       " 11.6770625,\n",
       " 18.55,\n",
       " 15.89,\n",
       " 13.5,\n",
       " 15.07,\n",
       " 15.87,\n",
       " 11.74,\n",
       " 15.32,\n",
       " 17.03,\n",
       " 19.92,\n",
       " 10.03,\n",
       " 14.01,\n",
       " 12.76,\n",
       " 16.07,\n",
       " 14.17,\n",
       " 12.15,\n",
       " 15.67,\n",
       " 13.74,\n",
       " 11.45,\n",
       " 11.69,\n",
       " 12.89,\n",
       " 15.05,\n",
       " 14.55,\n",
       " 10.12,\n",
       " 18.22,\n",
       " 12.8,\n",
       " 19.96,\n",
       " 14.41,\n",
       " 16.93,\n",
       " 16.11,\n",
       " 12.51,\n",
       " 18.02,\n",
       " 18.78,\n",
       " 19.35,\n",
       " 19.47,\n",
       " 19.58,\n",
       " 14.03,\n",
       " 11.89,\n",
       " 14.7,\n",
       " 15.52,\n",
       " 19.79,\n",
       " 12.79,\n",
       " 11.61,\n",
       " 14.19,\n",
       " 10.01,\n",
       " 16.16,\n",
       " 14.26,\n",
       " 16.66,\n",
       " 14.41,\n",
       " 19.44,\n",
       " 14.79,\n",
       " 10.38,\n",
       " 17.84,\n",
       " 12.3,\n",
       " 10.08,\n",
       " 16.39,\n",
       " 16.61,\n",
       " 10.97,\n",
       " 14.16,\n",
       " 15.87,\n",
       " 15.95,\n",
       " 16.07,\n",
       " 16.91,\n",
       " 10.04,\n",
       " 15.82,\n",
       " 18.2581875,\n",
       " 19.65,\n",
       " 16.03,\n",
       " 14.65,\n",
       " 13.61,\n",
       " 16.01,\n",
       " 18.66,\n",
       " 12.24,\n",
       " 18.85,\n",
       " 16.65,\n",
       " 15.53,\n",
       " 11.15,\n",
       " 13.21,\n",
       " 19.45,\n",
       " 11.19,\n",
       " 13.76,\n",
       " 18.71,\n",
       " 14.77,\n",
       " 16.43,\n",
       " 11.75,\n",
       " 10.04,\n",
       " 17.38,\n",
       " 14.59,\n",
       " 16.95,\n",
       " 15.28,\n",
       " 15.67,\n",
       " 10.84,\n",
       " 12.77,\n",
       " 17.73,\n",
       " 16.62,\n",
       " 10.73,\n",
       " 16.44,\n",
       " 13.54,\n",
       " 19.12,\n",
       " 15.42,\n",
       " 13.62,\n",
       " 18.68,\n",
       " 12.69,\n",
       " 15.44,\n",
       " 17.07,\n",
       " 16.72,\n",
       " 10.58,\n",
       " 18.54,\n",
       " 19.88,\n",
       " 10.93,\n",
       " 14.98,\n",
       " 17.1,\n",
       " 11.38,\n",
       " 14.71,\n",
       " 17.85,\n",
       " 14.92,\n",
       " 15.01,\n",
       " 17.71,\n",
       " 14.78,\n",
       " 17.72,\n",
       " 15.13,\n",
       " 13.44,\n",
       " 18.36,\n",
       " 19.19,\n",
       " 17.81,\n",
       " 18.36,\n",
       " 17.17,\n",
       " 16.9,\n",
       " 19.16,\n",
       " 15.85,\n",
       " 11.71,\n",
       " 15.77,\n",
       " 12.68,\n",
       " 13.96,\n",
       " 19.46,\n",
       " 16.01,\n",
       " 11.37,\n",
       " 15.15,\n",
       " 11.97,\n",
       " 13.37,\n",
       " 19.98,\n",
       " 15.95,\n",
       " 12.11,\n",
       " 10.24,\n",
       " 10.89,\n",
       " 11.64,\n",
       " 12.76,\n",
       " 19.96,\n",
       " 16.58,\n",
       " 11.61,\n",
       " 14.24,\n",
       " 18.04,\n",
       " 14.09,\n",
       " 12.76,\n",
       " 14.86,\n",
       " 16.2,\n",
       " 17.62,\n",
       " 14.41,\n",
       " 14.87,\n",
       " 19.5,\n",
       " 15.76,\n",
       " 16.61,\n",
       " 12.43,\n",
       " 12.02,\n",
       " 12.02,\n",
       " 16.57,\n",
       " 13.28,\n",
       " 12.23,\n",
       " 14.72,\n",
       " 17.11,\n",
       " 18.67,\n",
       " 16.49,\n",
       " 18.44,\n",
       " 11.68,\n",
       " 15.37,\n",
       " 14.52,\n",
       " 13.4,\n",
       " 13.06,\n",
       " 18.37,\n",
       " 16.89,\n",
       " 14.78,\n",
       " 13.06,\n",
       " 12.11,\n",
       " 15.29,\n",
       " 14.3,\n",
       " 13.5,\n",
       " 14.52,\n",
       " 16.8,\n",
       " 15.2,\n",
       " 12.81,\n",
       " 11.88,\n",
       " 13.6,\n",
       " 13.17,\n",
       " 12.17,\n",
       " 14.45,\n",
       " 13.0,\n",
       " 13.89,\n",
       " 11.17,\n",
       " 14.57,\n",
       " 17.99,\n",
       " 15.9,\n",
       " 19.91,\n",
       " 12.29,\n",
       " 10.25,\n",
       " 11.18,\n",
       " 18.12,\n",
       " 11.52,\n",
       " 14.65,\n",
       " 14.92,\n",
       " 11.61,\n",
       " 17.16,\n",
       " 16.97,\n",
       " 13.58,\n",
       " 11.39,\n",
       " 17.59,\n",
       " 17.19,\n",
       " 10.75,\n",
       " 18.27,\n",
       " 18.66,\n",
       " 19.27,\n",
       " 19.27,\n",
       " 13.11,\n",
       " 19.93,\n",
       " 10.33,\n",
       " 19.05,\n",
       " 19.93,\n",
       " 17.29,\n",
       " 14.96,\n",
       " 18.92,\n",
       " 13.53,\n",
       " 14.45,\n",
       " 16.91,\n",
       " 17.36,\n",
       " 12.07,\n",
       " 15.37,\n",
       " 11.2,\n",
       " 18.12,\n",
       " 17.65,\n",
       " 11.73,\n",
       " 16.99,\n",
       " 13.77,\n",
       " 18.53,\n",
       " 10.14,\n",
       " 12.47,\n",
       " 19.37,\n",
       " 14.28,\n",
       " 14.48,\n",
       " 16.1,\n",
       " 16.15,\n",
       " 15.78,\n",
       " 11.56,\n",
       " 10.89,\n",
       " 17.08,\n",
       " 13.14,\n",
       " 16.13,\n",
       " 14.89,\n",
       " 12.02,\n",
       " 11.11,\n",
       " 10.55,\n",
       " 12.03,\n",
       " 16.89,\n",
       " 19.61,\n",
       " 11.35,\n",
       " 17.75,\n",
       " 10.06,\n",
       " 19.35,\n",
       " 14.45,\n",
       " 17.03,\n",
       " 17.36,\n",
       " 14.62,\n",
       " 12.96,\n",
       " 11.3,\n",
       " 15.25,\n",
       " 13.34,\n",
       " 13.14,\n",
       " 15.38,\n",
       " 18.18,\n",
       " 13.33,\n",
       " 11.5,\n",
       " 16.15,\n",
       " 12.28,\n",
       " 17.98,\n",
       " 17.39,\n",
       " 12.23,\n",
       " 18.0,\n",
       " 10.22,\n",
       " 19.46,\n",
       " 12.14,\n",
       " 14.87,\n",
       " 13.25,\n",
       " 18.43,\n",
       " 12.46,\n",
       " 15.43,\n",
       " 17.37,\n",
       " 11.56,\n",
       " 12.22,\n",
       " 17.18,\n",
       " 16.37,\n",
       " 16.62,\n",
       " 19.47,\n",
       " 17.13,\n",
       " 12.67,\n",
       " 19.65,\n",
       " 12.15,\n",
       " 15.73,\n",
       " 17.66,\n",
       " 15.42,\n",
       " 15.84,\n",
       " 16.66,\n",
       " 17.87,\n",
       " 16.55,\n",
       " 16.8,\n",
       " 18.66,\n",
       " 14.08,\n",
       " 17.87,\n",
       " 14.43,\n",
       " 11.41,\n",
       " 17.46,\n",
       " 19.49,\n",
       " 19.46,\n",
       " 12.74,\n",
       " 17.12,\n",
       " 16.81,\n",
       " 14.74,\n",
       " 18.08,\n",
       " 11.39,\n",
       " 18.2,\n",
       " 16.44,\n",
       " 15.38,\n",
       " 18.84,\n",
       " 15.55,\n",
       " 16.18,\n",
       " 18.14,\n",
       " 10.27,\n",
       " 10.59,\n",
       " 18.6,\n",
       " 13.51,\n",
       " 13.36,\n",
       " 17.16,\n",
       " 18.93,\n",
       " 18.36725,\n",
       " 18.67,\n",
       " 18.22,\n",
       " 10.94,\n",
       " 16.08,\n",
       " 16.23,\n",
       " 18.78,\n",
       " 14.84,\n",
       " 11.47,\n",
       " 18.95,\n",
       " 14.3,\n",
       " 11.84,\n",
       " 12.21,\n",
       " 18.79,\n",
       " 15.65,\n",
       " 15.87,\n",
       " 15.75,\n",
       " 18.33,\n",
       " 18.88,\n",
       " 18.45,\n",
       " 11.11,\n",
       " 15.47,\n",
       " 13.87,\n",
       " 18.73,\n",
       " 19.64,\n",
       " 13.48,\n",
       " 10.63,\n",
       " 15.96,\n",
       " 15.69,\n",
       " 13.33,\n",
       " 16.36,\n",
       " 10.88,\n",
       " 16.74,\n",
       " 11.62,\n",
       " 14.01,\n",
       " 14.63,\n",
       " 11.5,\n",
       " 18.22,\n",
       " 19.99,\n",
       " 15.74,\n",
       " 12.17,\n",
       " 19.43,\n",
       " 16.62,\n",
       " 13.47,\n",
       " 12.12,\n",
       " 19.57,\n",
       " 18.82,\n",
       " 18.55,\n",
       " 16.52,\n",
       " 12.21,\n",
       " 14.42,\n",
       " 17.59,\n",
       " 14.21,\n",
       " 10.57,\n",
       " 13.4,\n",
       " 13.61,\n",
       " 16.31,\n",
       " 16.31,\n",
       " 10.69,\n",
       " 13.16,\n",
       " 15.29,\n",
       " 18.93,\n",
       " 11.06,\n",
       " 14.92,\n",
       " 16.34,\n",
       " 15.15,\n",
       " 19.11,\n",
       " 18.38,\n",
       " 16.98,\n",
       " 11.98,\n",
       " 12.9,\n",
       " 15.4,\n",
       " 18.63,\n",
       " 11.44,\n",
       " 17.87,\n",
       " 14.12,\n",
       " 13.07,\n",
       " 17.6,\n",
       " 15.06,\n",
       " 15.81,\n",
       " 17.51,\n",
       " 14.76,\n",
       " 10.87,\n",
       " 17.38,\n",
       " 12.41,\n",
       " 14.86,\n",
       " 19.95,\n",
       " 12.98,\n",
       " 15.28,\n",
       " 16.74,\n",
       " 10.22,\n",
       " 16.59,\n",
       " 13.34,\n",
       " 13.76,\n",
       " 17.75,\n",
       " 18.06,\n",
       " 13.33,\n",
       " 16.71,\n",
       " 10.2,\n",
       " 16.89,\n",
       " 16.94,\n",
       " 13.62,\n",
       " 17.68,\n",
       " 13.12,\n",
       " 10.92,\n",
       " 16.5,\n",
       " 18.04,\n",
       " 15.73,\n",
       " 11.02,\n",
       " 11.85,\n",
       " 19.39,\n",
       " 18.26,\n",
       " 17.82,\n",
       " 16.41,\n",
       " 18.15,\n",
       " 16.03,\n",
       " 14.21,\n",
       " 18.8064375,\n",
       " 14.61,\n",
       " 18.81,\n",
       " 11.95,\n",
       " 17.85,\n",
       " 14.31,\n",
       " 15.39,\n",
       " 17.95,\n",
       " 14.53,\n",
       " 17.45,\n",
       " 14.73,\n",
       " 17.22,\n",
       " 13.89,\n",
       " 18.72,\n",
       " 13.7,\n",
       " 18.3,\n",
       " 15.69,\n",
       " 12.8,\n",
       " 11.17,\n",
       " 12.86,\n",
       " 11.49,\n",
       " 20.0,\n",
       " 15.35,\n",
       " 12.17,\n",
       " 17.87,\n",
       " 11.84,\n",
       " 11.6,\n",
       " 12.49,\n",
       " 16.99,\n",
       " 16.52,\n",
       " 14.56,\n",
       " 13.45,\n",
       " 11.74,\n",
       " 14.55,\n",
       " 12.45,\n",
       " 14.79,\n",
       " 14.36,\n",
       " 13.11,\n",
       " 15.86,\n",
       " 16.61,\n",
       " 13.83,\n",
       " 12.46,\n",
       " 15.68,\n",
       " 18.09,\n",
       " 10.63,\n",
       " 10.19,\n",
       " 16.96,\n",
       " 10.55,\n",
       " 10.68,\n",
       " 20.0,\n",
       " 12.31,\n",
       " 13.82,\n",
       " 10.38,\n",
       " 10.41,\n",
       " 14.43,\n",
       " 12.48,\n",
       " 20.0,\n",
       " 12.91,\n",
       " 12.02,\n",
       " 14.58,\n",
       " 16.15,\n",
       " 11.53,\n",
       " 16.99,\n",
       " 20.0,\n",
       " 17.61,\n",
       " 18.5,\n",
       " 19.7,\n",
       " 14.71,\n",
       " 18.45,\n",
       " 16.77,\n",
       " 17.21,\n",
       " 13.78,\n",
       " 15.16,\n",
       " 18.29,\n",
       " 15.56,\n",
       " 13.18,\n",
       " 15.72,\n",
       " 10.79,\n",
       " 10.64,\n",
       " 19.19,\n",
       " 15.27,\n",
       " 10.69,\n",
       " 19.98,\n",
       " 18.9,\n",
       " 10.72,\n",
       " 17.64,\n",
       " 12.91,\n",
       " 10.98,\n",
       " 18.4,\n",
       " 14.37,\n",
       " 17.24,\n",
       " 10.94,\n",
       " 12.77,\n",
       " 11.71,\n",
       " 15.13,\n",
       " 17.9,\n",
       " 10.72,\n",
       " 11.9,\n",
       " 18.12,\n",
       " 11.86,\n",
       " 20.0,\n",
       " 10.7,\n",
       " 16.69,\n",
       " 12.68,\n",
       " 12.32,\n",
       " 11.2,\n",
       " 17.73,\n",
       " 14.87,\n",
       " 17.37,\n",
       " 14.63,\n",
       " 15.07,\n",
       " 13.09,\n",
       " 14.19,\n",
       " 15.81,\n",
       " 12.82,\n",
       " 11.24,\n",
       " 10.56,\n",
       " 20.0,\n",
       " 13.83,\n",
       " 12.48,\n",
       " 14.94,\n",
       " 12.18]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x / 16000 for x in dataset['train']['input_length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['train'] = dataset['train'].take(1000).map(prepare_dataset, num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['train'] = dataset['train'].take(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = raw_dataset.remove_columns(['audio', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.load lora model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 distil-whisper 模型\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"distil-whisper/distil-medium.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.decoder.layers.0.self_attn.k_proj',\n",
       " 'model.decoder.layers.0.self_attn.v_proj',\n",
       " 'model.decoder.layers.0.self_attn.q_proj',\n",
       " 'model.decoder.layers.0.self_attn.out_proj',\n",
       " 'model.decoder.layers.0.self_attn_layer_norm',\n",
       " 'model.decoder.layers.0.encoder_attn.k_proj',\n",
       " 'model.decoder.layers.0.encoder_attn.v_proj',\n",
       " 'model.decoder.layers.0.encoder_attn.q_proj',\n",
       " 'model.decoder.layers.0.encoder_attn.out_proj',\n",
       " 'model.decoder.layers.0.encoder_attn_layer_norm',\n",
       " 'model.decoder.layers.0.fc1',\n",
       " 'model.decoder.layers.0.fc2',\n",
       " 'model.decoder.layers.0.final_layer_norm',\n",
       " 'model.decoder.layers.1.self_attn.k_proj',\n",
       " 'model.decoder.layers.1.self_attn.v_proj',\n",
       " 'model.decoder.layers.1.self_attn.q_proj',\n",
       " 'model.decoder.layers.1.self_attn.out_proj',\n",
       " 'model.decoder.layers.1.self_attn_layer_norm',\n",
       " 'model.decoder.layers.1.encoder_attn.k_proj',\n",
       " 'model.decoder.layers.1.encoder_attn.v_proj',\n",
       " 'model.decoder.layers.1.encoder_attn.q_proj',\n",
       " 'model.decoder.layers.1.encoder_attn.out_proj',\n",
       " 'model.decoder.layers.1.encoder_attn_layer_norm',\n",
       " 'model.decoder.layers.1.fc1',\n",
       " 'model.decoder.layers.1.fc2',\n",
       " 'model.decoder.layers.1.final_layer_norm',\n",
       " 'model.decoder.layer_norm']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_modules = []\n",
    "keywords = [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"]\n",
    "for id, (name, param) in enumerate(model.named_modules()):\n",
    "    if 'model.decoder' in name and (any(keyword in name for keyword in keywords)):\n",
    "        target_modules.append(name)\n",
    "        \n",
    "target_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 LoRA 配置\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                    # Rank 参数\n",
    "    lora_alpha=32,           # alpha乘数\n",
    "    target_modules=target_modules,  # 目标模块，应用LoRA的部分\n",
    "    lora_dropout=0.1,        # dropout概率\n",
    "    bias=\"none\",             # 不应用到 bias\n",
    ")\n",
    "\n",
    "# 将 LoRA 配置应用到模型\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 425,984 || all params: 394,801,152 || trainable%: 0.1079\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_layers = filter(lambda p: p.requires_grad, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<filter at 0x73e542657670>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425984"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425984"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for group in optimizer.param_groups for p in group['params'] if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpds1/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./model\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=1000,\n",
    "    # max_steps=100, # only for testing purposes, remove this from your final run :)\n",
    "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
    "    label_names=[\"labels\"],  # same reason as above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor ([`Wav2Vec2Processor`])\n",
    "            The processor used for proccessing the data.\n",
    "        decoder_start_token_id (:obj: `int`)\n",
    "            The start-of-sequence token id of the decoder.\n",
    "        decoder_prev_token_id (:obj: `int`)\n",
    "            The start-of-prompt token id of the decoder\n",
    "        input_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned input sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        target_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned target sequences (according to the model's padding side and padding index).\n",
    "            See above for details.\n",
    "        max_target_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` of the returned list and optionally padding length (see above).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "    decoder_prev_token_id: int\n",
    "    input_padding: Union[bool, str] = \"max_length\"\n",
    "    target_padding: Union[bool, str] = \"max_length\"\n",
    "    max_target_length: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], np.ndarray]]]) -> Dict[str, np.ndarray]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "\n",
    "        # dataloader returns a list of features which we convert to a dict\n",
    "        input_features = {\"input_features\": [feature[\"input_features\"] for feature in features]}\n",
    "        label_features = {\"input_ids\": [feature[\"labels\"] for feature in features]}\n",
    "\n",
    "        # reformat list to dict and set to pytorch format\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.input_padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            max_length=self.max_target_length,\n",
    "            padding=self.target_padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # shift labels to the right to get decoder input ids\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "        decoder_input_ids = labels[:, :-1]\n",
    "        labels = labels[:, 1:]\n",
    "        labels_mask = labels_batch.attention_mask[:, 1:]\n",
    "\n",
    "        # replace padding with -100 to ignore correctly when computing the loss\n",
    "        labels = labels.masked_fill(labels_mask.ne(1), -100)\n",
    "\n",
    "        # replace initial prompt tokens with -100 to ignore correctly when computing the loss\n",
    "        bos_index = torch.argmax((labels == self.decoder_start_token_id).long(), dim=1)\n",
    "        bos_index = torch.where(bos_index > 0, bos_index + 1, bos_index)\n",
    "        prompt_mask = torch.arange(labels.shape[1]) < bos_index[:, None]\n",
    "        labels = torch.where(prompt_mask, -100, labels)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"decoder_input_ids\"] = decoder_input_ids\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import preprocess_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"./checkpoint-25000-epoch-1\" # Use the same model ID as before.\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    peft_config.base_model_name_or_path\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): WhisperForConditionalGeneration(\n",
       "      (model): WhisperModel(\n",
       "        (encoder): WhisperEncoder(\n",
       "          (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (embed_positions): Embedding(1500, 1024)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x WhisperEncoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): WhisperDecoder(\n",
       "          (embed_tokens): Embedding(51864, 1024, padding_idx=50256)\n",
       "          (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x WhisperDecoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): WhisperSdpaAttention(\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=1024, out_features=51864, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0edfe907a3048f9910df7b88b48073c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc18535163140e09888fb3885f15ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0588b02428d84ce29851a0e67895acfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = preprocess_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'text', 'input_features', 'input_length', 'labels'],\n",
       "        num_rows: 1061465\n",
       "    })\n",
       "    ID_eval: Dataset({\n",
       "        features: ['audio', 'text', 'input_features', 'input_length', 'labels'],\n",
       "        num_rows: 19567\n",
       "    })\n",
       "    OOD_eval: Dataset({\n",
       "        features: ['audio', 'text', 'input_features', 'input_length', 'labels'],\n",
       "        num_rows: 4077\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"openai/whisper-medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(base_model)\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_start_token_id = model.config.decoder_start_token_id  # <|startoftranscript|>\n",
    "decoder_prev_token_id = tokenizer.all_special_ids[-3]  # <|startofprev|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=decoder_start_token_id,\n",
    "    decoder_prev_token_id=decoder_prev_token_id,\n",
    "    input_padding=\"longest\",\n",
    "    target_padding=\"max_length\",\n",
    "    max_target_length=448,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'text', 'input_features', 'input_length', 'labels'],\n",
       "        num_rows: 1061465\n",
       "    })\n",
       "    ID_eval: Dataset({\n",
       "        features: ['audio', 'text', 'input_features', 'input_length', 'labels'],\n",
       "        num_rows: 19567\n",
       "    })\n",
       "    OOD_eval: Dataset({\n",
       "        features: ['audio', 'text', 'input_features', 'input_length', 'labels'],\n",
       "        num_rows: 4077\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset['train'],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=72,\n",
    "    drop_last=False,\n",
    "    num_workers=8,\n",
    "    # pin_memory=training_args.dataloader_pin_memory,\n",
    ")\n",
    "\n",
    "ID_dataloader = DataLoader(\n",
    "    dataset['ID_eval'],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=72,\n",
    "    drop_last=False,\n",
    "    num_workers=8,\n",
    "    # pin_memory=training_args.dataloader_pin_memory,\n",
    ")\n",
    "\n",
    "OOD_dataloader = DataLoader(\n",
    "    dataset['OOD_eval'],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=72,\n",
    "    drop_last=False,\n",
    "    num_workers=8,\n",
    "    # pin_memory=training_args.dataloader_pin_memory,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"max_length\": 128,\n",
    "    \"num_beams\": 5,\n",
    "    # \"language\": 'de', \n",
    "    # \"task\": 'transcription',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa45f24254f41fdb001617b6c330ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating train_eval...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_predictions = []\n",
    "train_references = []\n",
    "train_normalized_predictions = []\n",
    "train_normalized_references = []\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "for batch in tqdm(\n",
    "    islice(train_dataloader, 100),\n",
    "    desc=f\"Evaluating {'train_eval'}...\",\n",
    "    ):\n",
    "    generated_ids = model.generate(batch[\"input_features\"].to('cuda'), **gen_kwargs)\n",
    "    labels = batch[\"labels\"]\n",
    "    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    train_predictions.extend(decoded_preds)\n",
    "    train_references.extend(decoded_labels)\n",
    "    train_normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
    "    train_normalized_references.extend([normalizer(label).strip() for label in decoded_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb03e0779294afb9c168110c827087c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ID_eval...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_predictions = []\n",
    "id_references = []\n",
    "id_normalized_predictions = []\n",
    "id_normalized_references = []\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "ID_dataloader = islice(ID_dataloader, 100)\n",
    "\n",
    "for batch in tqdm(\n",
    "    ID_dataloader,\n",
    "    desc=f\"Evaluating {'ID_eval'}...\",\n",
    "    ):\n",
    "    generated_ids = model.generate(batch[\"input_features\"].to(\"cuda\"), **gen_kwargs)\n",
    "    labels = batch[\"labels\"]\n",
    "    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    id_predictions.extend(decoded_preds)\n",
    "    id_references.extend(decoded_labels)\n",
    "    id_normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
    "    id_normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n",
    "    # del generated_ids, labels, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4eaff25cf049af988b377da8e8f75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating OOD_eval...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ood_predictions = []\n",
    "ood_references = []\n",
    "ood_normalized_predictions = []\n",
    "ood_normalized_references = []\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "OOD_dataloader = islice(OOD_dataloader, 100)\n",
    "\n",
    "for batch in tqdm(\n",
    "    OOD_dataloader,\n",
    "    desc=f\"Evaluating {'OOD_eval'}...\",\n",
    "    ):\n",
    "    generated_ids = model.generate(batch[\"input_features\"].to('cuda'), **gen_kwargs)\n",
    "    labels = batch[\"labels\"]\n",
    "    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    ood_predictions.extend(decoded_preds)\n",
    "    ood_references.extend(decoded_labels)\n",
    "    ood_normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
    "    ood_normalized_references.extend([normalizer(label).strip() for label in decoded_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: WER: 29.05259283117839, Normalized WER: 27.846659532181622\n"
     ]
    }
   ],
   "source": [
    "wer = 100 * metric.compute(predictions=train_predictions, references=train_references)\n",
    "normalized_wer = 100 * metric.compute(predictions=train_normalized_predictions, references=train_normalized_references)\n",
    "\n",
    "print(f\"train: WER: {wer}, Normalized WER: {normalized_wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 38.64809810011698, Normalized WER: 36.10389349812899\n"
     ]
    }
   ],
   "source": [
    "wer = 100 * metric.compute(predictions=id_predictions, references=id_references)\n",
    "normalized_wer = 100 * metric.compute(predictions=id_normalized_predictions, references=id_normalized_references)\n",
    "\n",
    "print(f\"ID: WER: {wer}, Normalized WER: {normalized_wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4077"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_predictions_new, ood_references_new = zip(*[(x, y) for x, y in zip(ood_predictions, ood_references) if x != \"\" and y != \"\"])\n",
    "ood_normalized_predictions_new, ood_normalized_references_new = zip(*[(x, y) for x, y in zip(ood_normalized_predictions, ood_normalized_references) if x != \"\" and y != \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOD: WER: 63.56350110984462, Normalized WER: 50.20510483135825\n"
     ]
    }
   ],
   "source": [
    "wer = 100 * metric.compute(predictions=ood_predictions_new, references=ood_references_new)\n",
    "normalized_wer = 100 * metric.compute(predictions=ood_normalized_predictions_new, references=ood_normalized_references_new)\n",
    "\n",
    "print(f\"OOD: WER: {wer}, Normalized WER: {normalized_wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"distil-whisper/distil-medium.en\").to(\"cuda\")\n",
    "\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(\"distil-whisper/distil-medium.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset['train'],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=72,\n",
    "    drop_last=False,\n",
    "    num_workers=8,\n",
    "    # pin_memory=training_args.dataloader_pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d9795f52e7402e9e86485a23d2645b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating train_eval...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 19\u001b[0m\n\u001b[1;32m      8\u001b[0m gen_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# \"language\": 'de', \u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# \"task\": 'transcription',\u001b[39;00m\n\u001b[1;32m     13\u001b[0m }\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     16\u001b[0m     islice(train_dataloader, \u001b[38;5;241m100\u001b[39m),\n\u001b[1;32m     17\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_eval\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     ):\n\u001b[0;32m---> 19\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     21\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, labels, tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:671\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate\u001b[0;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, return_token_timestamps, return_segments, return_dict_in_generate, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m             proc\u001b[38;5;241m.\u001b[39mset_begin_index(decoder_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    664\u001b[0m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[1;32m    665\u001b[0m (\n\u001b[1;32m    666\u001b[0m     seek_sequences,\n\u001b[1;32m    667\u001b[0m     seek_outputs,\n\u001b[1;32m    668\u001b[0m     should_skip,\n\u001b[1;32m    669\u001b[0m     do_condition_on_prev_tokens,\n\u001b[1;32m    670\u001b[0m     model_output_type,\n\u001b[0;32m--> 671\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:832\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate_with_fallback\u001b[0;34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    829\u001b[0m             generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, batch_size \u001b[38;5;241m-\u001b[39m cur_bsz), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    830\u001b[0m         )\n\u001b[0;32m--> 832\u001b[0m seek_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m model_output_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/transformers/generation/utils.py:2063\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2055\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2056\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2057\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2058\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2059\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2060\u001b[0m     )\n\u001b[1;32m   2062\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2063\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2076\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2077\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2078\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2084\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2085\u001b[0m     )\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/transformers/generation/utils.py:3251\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   3246\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m   3247\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[1;32m   3248\u001b[0m     next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3249\u001b[0m )  \u001b[38;5;66;03m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[0;32m-> 3251\u001b[0m next_token_scores_processed \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_sample:\n\u001b[1;32m   3253\u001b[0m     next_token_scores_processed \u001b[38;5;241m=\u001b[39m logits_warper(input_ids, next_token_scores_processed)\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/transformers/generation/logits_process.py:98\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/transformers/generation/logits_process.py:1836\u001b[0m, in \u001b[0;36mSuppressTokensLogitsProcessor.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;129m@add_start_docstrings\u001b[39m(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m   1835\u001b[0m     vocab_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mscores\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1836\u001b[0m     suppress_token_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppress_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1837\u001b[0m     scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(suppress_token_mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m), scores)\n\u001b[1;32m   1838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_predictions = []\n",
    "train_references = []\n",
    "train_normalized_predictions = []\n",
    "train_normalized_references = []\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "gen_kwargs = {\n",
    "    \"max_length\": 128,\n",
    "    \"num_beams\": 5,\n",
    "    # \"language\": 'de', \n",
    "    # \"task\": 'transcription',\n",
    "}\n",
    "\n",
    "for batch in tqdm(\n",
    "    islice(train_dataloader, 100),\n",
    "    desc=f\"Evaluating {'train_eval'}...\",\n",
    "    ):\n",
    "    generated_ids = model.generate(batch[\"input_features\"].to('cuda'), **gen_kwargs)\n",
    "    labels = batch[\"labels\"]\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    train_predictions.extend(decoded_preds)\n",
    "    train_references.extend(decoded_labels)\n",
    "    train_normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
    "    train_normalized_references.extend([normalizer(label).strip() for label in decoded_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: WER: 113.99646630496858, Normalized WER: 118.0535665044787\n"
     ]
    }
   ],
   "source": [
    "wer = 100 * metric.compute(predictions=train_predictions, references=train_references)\n",
    "normalized_wer = 100 * metric.compute(predictions=train_normalized_predictions, references=train_normalized_references)\n",
    "\n",
    "print(f\"train: WER: {wer}, Normalized WER: {normalized_wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\").to(\"cpu\")\n",
    "model2 = WhisperForConditionalGeneration.from_pretrained(\"distil-whisper/distil-medium.en\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(51865, 1024, padding_idx=50257)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.model.decoder.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperConfig {\n",
       "  \"_name_or_path\": \"distil-whisper/distil-medium.en\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"apply_spec_augment\": false,\n",
       "  \"architectures\": [\n",
       "    \"WhisperForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"begin_suppress_tokens\": [\n",
       "    220,\n",
       "    50256\n",
       "  ],\n",
       "  \"bos_token_id\": 50257,\n",
       "  \"classifier_proj_size\": 256,\n",
       "  \"d_model\": 1024,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 4096,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 2,\n",
       "  \"decoder_start_token_id\": 50257,\n",
       "  \"dropout\": 0.0,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 4096,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 24,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"forced_decoder_ids\": [\n",
       "    [\n",
       "      1,\n",
       "      50362\n",
       "    ]\n",
       "  ],\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"mask_feature_length\": 10,\n",
       "  \"mask_feature_min_masks\": 0,\n",
       "  \"mask_feature_prob\": 0.0,\n",
       "  \"mask_time_length\": 10,\n",
       "  \"mask_time_min_masks\": 2,\n",
       "  \"mask_time_prob\": 0.05,\n",
       "  \"max_length\": 448,\n",
       "  \"max_source_positions\": 1500,\n",
       "  \"max_target_positions\": 448,\n",
       "  \"median_filter_width\": 7,\n",
       "  \"model_type\": \"whisper\",\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"num_mel_bins\": 80,\n",
       "  \"pad_token_id\": 50256,\n",
       "  \"scale_embedding\": false,\n",
       "  \"suppress_tokens\": [\n",
       "    1,\n",
       "    2,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    14,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    31,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    357,\n",
       "    366,\n",
       "    438,\n",
       "    532,\n",
       "    685,\n",
       "    705,\n",
       "    796,\n",
       "    930,\n",
       "    1058,\n",
       "    1220,\n",
       "    1267,\n",
       "    1279,\n",
       "    1303,\n",
       "    1343,\n",
       "    1377,\n",
       "    1391,\n",
       "    1635,\n",
       "    1782,\n",
       "    1875,\n",
       "    2162,\n",
       "    2361,\n",
       "    2488,\n",
       "    3467,\n",
       "    4008,\n",
       "    4211,\n",
       "    4600,\n",
       "    4808,\n",
       "    5299,\n",
       "    5855,\n",
       "    6329,\n",
       "    7203,\n",
       "    9609,\n",
       "    9959,\n",
       "    10563,\n",
       "    10786,\n",
       "    11420,\n",
       "    11709,\n",
       "    11907,\n",
       "    13163,\n",
       "    13697,\n",
       "    13700,\n",
       "    14808,\n",
       "    15306,\n",
       "    16410,\n",
       "    16791,\n",
       "    17992,\n",
       "    19203,\n",
       "    19510,\n",
       "    20724,\n",
       "    22305,\n",
       "    22935,\n",
       "    27007,\n",
       "    30109,\n",
       "    30420,\n",
       "    33409,\n",
       "    34949,\n",
       "    40283,\n",
       "    40493,\n",
       "    40549,\n",
       "    47282,\n",
       "    49146,\n",
       "    50257,\n",
       "    50357,\n",
       "    50358,\n",
       "    50359,\n",
       "    50360,\n",
       "    50361\n",
       "  ],\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.44.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_weighted_layer_sum\": false,\n",
       "  \"vocab_size\": 51864\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51864, 1024, padding_idx=50256)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1024, out_features=51864, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = WhisperTokenizerFast.from_pretrained(\"openai/whisper-medium\")\n",
    "tokenizer2 = WhisperTokenizerFast.from_pretrained(\"openai/whisper-medium.en\")\n",
    "tokenizer3 = WhisperTokenizerFast.from_pretrained(\"distil-whisper/distil-medium.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer4 = WhisperTokenizerFast.from_pretrained(\"openai/whisper-large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " 'ĠìļĶì¦ĺ',\n",
       " 'Ġgleich',\n",
       " 'Ġludzi',\n",
       " 'Ãĩ',\n",
       " 'Ġez',\n",
       " 'Ġresponder',\n",
       " 'ĠÃ©conomique',\n",
       " 'ĠnasÄ±l',\n",
       " 'ĠÐ½ÑĢÐ°Ð²',\n",
       " 'Ġbutts',\n",
       " 'Ġìĥī',\n",
       " 'æĹ¥',\n",
       " 'ÑĭÐµ',\n",
       " 'éĤĦæľī',\n",
       " 'ĠÑģÐ¾Ð²',\n",
       " 'ĠOke',\n",
       " 'owym',\n",
       " 'ĠÐ±Ñĭ',\n",
       " 'aphrag',\n",
       " 'ë¥',\n",
       " 'Ġë§ŀìķĦìļĶ',\n",
       " 'ĠÐ³Ð¾ÑĢÐ¾Ð´Ð°',\n",
       " 'Ġleva',\n",
       " 'ìħĺ',\n",
       " 'ìĹĲê²Į',\n",
       " 'Ã¡b',\n",
       " 'Ġtes',\n",
       " 'Ġbastards',\n",
       " 'ÐµÐºÑĤ',\n",
       " 'è¹',\n",
       " 'ĠJeg',\n",
       " 'Ġpasst',\n",
       " 'æĽ¸',\n",
       " 'çĽ¸',\n",
       " 'Ġging',\n",
       " 'Ġsegue',\n",
       " 'è®ĵ',\n",
       " 'ãģ¾ãģĽ',\n",
       " 'ÐµÑĢÑĥ',\n",
       " 'ĠìĻĢ',\n",
       " 'Ġë¹Ħ',\n",
       " 'Ġirrespons',\n",
       " 'ÐĹÐ´',\n",
       " 'ĠItÃŃs',\n",
       " 'onya',\n",
       " 'Ġbana',\n",
       " 'ĠØ¨Øª',\n",
       " 'ĠÐĹÐ°',\n",
       " 'ç´¯',\n",
       " 'íķľëį°',\n",
       " 'riÃ¨re',\n",
       " 'ĠAvo',\n",
       " 'ãĤĪãģĨ',\n",
       " '?...',\n",
       " 'ĠÐ·Ð°Ð´',\n",
       " 'ãģĿãģĹãģ¦',\n",
       " 'ÑĪÑĮ',\n",
       " 'æ¶',\n",
       " 'ĠDlatego',\n",
       " 'Ġhu',\n",
       " 'atif',\n",
       " 'ÏĢÏĮÎ½',\n",
       " 'ÃŃas',\n",
       " 'Ġprobabil',\n",
       " 'ê²łë',\n",
       " 'quoi',\n",
       " 'ìĿ¸ìĿĦ',\n",
       " 'Ġëĵ¤ìĸ´ë',\n",
       " 'ÐµÑĢÑĤÐ²',\n",
       " 'Ã¢nd',\n",
       " 'ĠdoÅĽwiad',\n",
       " 'Ġimaginar',\n",
       " 'Ġblev',\n",
       " 'Ã¦r',\n",
       " 'Ã§on',\n",
       " 'Ġgogg',\n",
       " 'erkt',\n",
       " 'Ġcontrat',\n",
       " 'ĠJeez',\n",
       " 'ĠSchÃ¶n',\n",
       " 'Ġwidz',\n",
       " 'ĠÐ¾ÑĤÐ²ÐµÑĩ',\n",
       " 'ĠëŃĲë',\n",
       " 'ĠëŃĶê°Ģ',\n",
       " 'Ġastronom',\n",
       " 'á»±c',\n",
       " 'ĠëĬĲë',\n",
       " 'uen',\n",
       " 'ãģĦãģĨãģĵãģ¨',\n",
       " 'Ð»Ð¾ÑģÑĮ',\n",
       " 'Ġih',\n",
       " 'ĠZiel',\n",
       " 'ÙĪØ§',\n",
       " 'ÏĨ',\n",
       " 'ĠÑĤÐµÑħ',\n",
       " 'ĠØ£ÙĬ',\n",
       " 'Ġ×Ķ×Ĳ×',\n",
       " 'sst',\n",
       " 'ìķħ',\n",
       " 'ĠÑĢÐ¾Ð»',\n",
       " 'Ġfalou',\n",
       " 'ì¹ĺë',\n",
       " 'ĠkÃ¶',\n",
       " 'erting',\n",
       " 'à¸µà¹ī',\n",
       " 'Ð°Ð½Ð¸Ñİ',\n",
       " 'Ġë§Īë¬´ë',\n",
       " 'è®',\n",
       " 'Ġdevam',\n",
       " 'ORIA',\n",
       " 'ëĺ',\n",
       " 'Ġprincipio',\n",
       " 'avaÅŁ',\n",
       " 'Ġanyhow',\n",
       " 'ìļ´ë',\n",
       " 'æĢª',\n",
       " 'ĠTensor',\n",
       " 'Ġsai',\n",
       " 'Ġexpres',\n",
       " 'ĠVISTA',\n",
       " 'æĻĤéĸĵ',\n",
       " 'ë§Ī',\n",
       " 'įĶëĿ¼ê³ł',\n",
       " 'Ġdesenvol',\n",
       " 'Ġargu',\n",
       " 'utar',\n",
       " 'çĽ´æİ¥',\n",
       " 'ĠØ§Ø³',\n",
       " 'ĠMaintenant',\n",
       " 'æĪĳè¦ģ',\n",
       " 'ĠfrÃ¼her',\n",
       " 'Ġ×Ķ×Ļ×Ĳ',\n",
       " 'ĠDaha',\n",
       " 'Ġgeschafft',\n",
       " 'ĠÐ²Ð¸Ð´ÐµÐ»Ð¸',\n",
       " 'Ġnacional',\n",
       " 'ĠSicht',\n",
       " 'Ġsist',\n",
       " 'Ġgaat',\n",
       " 'Ð°ÑģÑĤÑĮ',\n",
       " 'Ġhacerlo',\n",
       " 'ĠÐ´Ð°Ð»ÑĮ',\n",
       " 'ningen',\n",
       " 'èĶ',\n",
       " 'ĠìĹ¬ê¸°ìĦľ',\n",
       " 'ĠÐ²Ð´',\n",
       " 'Ġprze',\n",
       " 'WAN',\n",
       " '¬ë¥¼',\n",
       " 'Ä±yÄ±',\n",
       " 'ÑĥÐ¹',\n",
       " 'Ġë³´ìĹ¬ëĵľë',\n",
       " 'ĠÎ±ÏħÏĦÏĮ',\n",
       " 'Ð°ÑĨ',\n",
       " 'ĠÐ¿Ð¾ÐºÐ°Ð·ÑĭÐ²',\n",
       " 'iÃ¨rement',\n",
       " 'Ġlemonade',\n",
       " 'plo',\n",
       " 'adora',\n",
       " 'Ð²Ð¸',\n",
       " 'ĠÐ¿ÑĢÐ°ÐºÑĤÐ¸',\n",
       " 'à®ķà®³',\n",
       " 'ĠÐ²Ð¾ÑĤ',\n",
       " 'Ġcosplay',\n",
       " 'elier',\n",
       " 'ĠWohn',\n",
       " 'ìĹĪëĬĶëį°',\n",
       " 'Ġ×Ķ×ª',\n",
       " 'ĠrzeczywiÅĽcie',\n",
       " 'ãħłãħł',\n",
       " 'ĠSovi',\n",
       " 'ë°ľ',\n",
       " 'Ġoscillator',\n",
       " 'ĠÐ¾ÑĤÐ²',\n",
       " 'Ġyar',\n",
       " 'ĠvidÃ©os',\n",
       " 'Ġtril',\n",
       " 'ĠJESS',\n",
       " 'ligt',\n",
       " 'venidos',\n",
       " 'ĠÐ½Ð°ÑģÑĤ',\n",
       " 'Ġllam',\n",
       " 'Ã¼hr',\n",
       " 'Ġãģ¾ãģĻ',\n",
       " 'åĵĪ',\n",
       " 'ĠKristin',\n",
       " '],,',\n",
       " 'ĠdifÃŃ',\n",
       " 'kap',\n",
       " 'ĠÙĪÙĦ',\n",
       " 'Ġtodas',\n",
       " 'Ġbeste',\n",
       " 'á»ĭch',\n",
       " 'ĠpÃ³Åºniej',\n",
       " 'Ġamar',\n",
       " 'ĠQuem',\n",
       " 'Ġbegr',\n",
       " 'ĦĪ',\n",
       " 'Ġbesonders',\n",
       " 'æ¡',\n",
       " 'è¡ĮäºĨ',\n",
       " 'Î²Î±',\n",
       " 'Ġencima',\n",
       " 'èĬĤ',\n",
       " 'Ġnecesita',\n",
       " 'Ġkull',\n",
       " 'ĠÑī',\n",
       " 'Ġpurl',\n",
       " 'ĠÐ¸Ð½ÑĤÐµÑĢÐµÑģÐ½Ð¾',\n",
       " 'Ġseus',\n",
       " 'ĠwspÃ³ÅĤ',\n",
       " 'çīĩ',\n",
       " 'Ã¡z',\n",
       " 'ĠÐ¾ÑĤÐ½Ð¾ÑĪ',\n",
       " 'Ð°ÑĢÑĤ',\n",
       " 'ĠGobierno',\n",
       " 'ughters',\n",
       " 'Ġëĭ¤',\n",
       " '!..',\n",
       " 'æ©ĭ',\n",
       " 'ĠrÃ©du',\n",
       " 'Ġstatt',\n",
       " 'Ð¾Ð¼',\n",
       " 'ç¢',\n",
       " 'Ð¼Ð¾Ñģ',\n",
       " 'Ġces',\n",
       " 'Ġassistir',\n",
       " 'xim',\n",
       " 'Ġìļ°ë¦¬ëĬĶ',\n",
       " 'ÑģÑĤÐ°ÑĤÐ¾ÑĩÐ½Ð¾',\n",
       " 'ichtlich',\n",
       " 'Ġpartir',\n",
       " 'Ð¸ÑĩÐµ',\n",
       " 'Ġkoy',\n",
       " 'çİ°åľ¨',\n",
       " 'avilion',\n",
       " 'ëª¨',\n",
       " 'å¹¹åĺĽ',\n",
       " 'ĠÐ½Ð¾Ð²ÑĭÐµ',\n",
       " 'çĽ´',\n",
       " 'ĠÄ°ns',\n",
       " 'cribing',\n",
       " 'Ġorchestral',\n",
       " 'Ġami',\n",
       " 'Ġbesten',\n",
       " 'Ð¾Ð¹Ð´',\n",
       " '×¢×Ŀ',\n",
       " 'Ġdiye',\n",
       " 'åĵĪåĵĪ',\n",
       " 'Ð¸Ð¼Ð¾ÑģÑĤÑĮ',\n",
       " 'å®ĥçļĦ',\n",
       " 'isÃ©s',\n",
       " 'ÅĤy',\n",
       " 'acaÄŁ',\n",
       " 'ĠØ§Ø¨',\n",
       " 'ĠPronunciation',\n",
       " 'erca',\n",
       " 'Ġmarc',\n",
       " 'anh',\n",
       " 'Ġauf',\n",
       " 'remo',\n",
       " 'ĠJoanna',\n",
       " 'ivat',\n",
       " 'ĠÙĦÙĥÙĨ',\n",
       " 'ĠkÃ¼',\n",
       " 'kaa',\n",
       " 'ĠHye',\n",
       " 'ĠÅŁeyler',\n",
       " 'ĠWhoo',\n",
       " 'Ġpon',\n",
       " 'owy',\n",
       " 'ĠÏĩ',\n",
       " 'ê²©',\n",
       " 'oncÃ©',\n",
       " 'ĠAirPods',\n",
       " 'Ġhacer',\n",
       " 'Ð»ÐµÑĤ',\n",
       " 'Ġpeine',\n",
       " 'å°ı',\n",
       " 'ĠÐ¡Ð¿',\n",
       " 'ĠKont',\n",
       " 'Ġprototy',\n",
       " 'Ġtud',\n",
       " 'istling',\n",
       " 'ihin',\n",
       " 'Ġdevenir',\n",
       " 'Ġbla',\n",
       " 'ãģĴ',\n",
       " 'icie',\n",
       " 'ĠØªÚ¾',\n",
       " 'ĠÑģÑĤÑĥÐ´',\n",
       " 'Ġbam',\n",
       " 'crates',\n",
       " 'Ð¾Ð·',\n",
       " 'ĠThats',\n",
       " 'Ġhypoth',\n",
       " 'Ġpermitir',\n",
       " 'Ġgla',\n",
       " 'Ġhandout',\n",
       " 'ÐµÐ¶Ð´',\n",
       " 'ÐµÐºÐ°',\n",
       " 'æĩĤ',\n",
       " 'inta',\n",
       " 'ĠÙĪØ¬',\n",
       " 'Ġreconoc',\n",
       " 'ÏĮ',\n",
       " 'Ġvita',\n",
       " 'Ã¡i',\n",
       " '×ķ×ł×Ķ',\n",
       " 'Ð¾Ð»Ð¶',\n",
       " 'å®Ł',\n",
       " 'ĠScandin',\n",
       " 'Ġë¿Įë',\n",
       " 'Ġìĵ¸',\n",
       " 'Ġ×Ĳ×ª×Ķ',\n",
       " 'cessors',\n",
       " 'Ġthermometer',\n",
       " 'Ġhvis',\n",
       " 'Ġparano',\n",
       " 'ëħĦ',\n",
       " 'Ġihm',\n",
       " 'æ¾',\n",
       " 'ä¸Ģä¸ĭ',\n",
       " 'ej',\n",
       " 'ĠìĬ¤íĥĢìĿ¼',\n",
       " 'Ġrigt',\n",
       " 'Ġcommunic',\n",
       " 'ĠtÄĻ',\n",
       " 'lk',\n",
       " 'ŀĺëıĦ',\n",
       " 'Ġë§ĪìĿĮìĹĲ',\n",
       " 'Ġempresas',\n",
       " 'ĠdÃ©couvrir',\n",
       " 'Ġapro',\n",
       " 'ĠMAL',\n",
       " 'à¸į',\n",
       " 'ãģĩ',\n",
       " 'mniej',\n",
       " 'Ġseni',\n",
       " 'ĥ½',\n",
       " 'ĠÐ²ÑĭÐ¿Ð¾Ð»',\n",
       " 'Ġconhecer',\n",
       " 'Ġleider',\n",
       " 'åĸľæŃ¡',\n",
       " 'ĠÐ½Ð°Ð±Ð»ÑİÐ´',\n",
       " 'Ġ×ľ×§',\n",
       " 'ĠRecht',\n",
       " 'Ġì½Ķ',\n",
       " '~\"',\n",
       " 'asmine',\n",
       " 'kannt',\n",
       " 'ungs',\n",
       " 'Ġolmak',\n",
       " 'alen',\n",
       " '(?)',\n",
       " 'ĠëĤ®',\n",
       " 'å½±',\n",
       " 'ĠdÃ¼ÅŁÃ¼n',\n",
       " 'Ġplante',\n",
       " 'Ġrespe',\n",
       " 'Ð°ÑģÑĮ',\n",
       " 'baar',\n",
       " 'Ġdua',\n",
       " 'Ġfamille',\n",
       " 'ĠvÃ¡',\n",
       " 'ĠmÃ©d',\n",
       " 'Ã©qu',\n",
       " 'ãģ¡ãĤĥ',\n",
       " 'ĠForgive',\n",
       " 'ĠprÃ©cis',\n",
       " 'Ġaj',\n",
       " 'Ġlequel',\n",
       " 'ĠÑħÐ¾Ñĩ',\n",
       " 'ĠìĹŃìĭľ',\n",
       " 'Audience',\n",
       " 'Ġdetta',\n",
       " 'Ġwaffle',\n",
       " 'Ġë°ĶëĢ',\n",
       " '´ì§Ģ',\n",
       " 'ÏĢÏĮ',\n",
       " 'iamente',\n",
       " 'Ġtanta',\n",
       " 'ëĵľë',\n",
       " 'Ġnatomiast',\n",
       " 'lais',\n",
       " 'æŃ²',\n",
       " 'ĠÐ¾ÑģÐ¾Ð±ÐµÐ½Ð½Ð¾',\n",
       " 'ĠìķĬìĿĢ',\n",
       " 'Ġpreguntas',\n",
       " 'åĢį',\n",
       " 'Ġentonces',\n",
       " 'ĠjeÅ¼eli',\n",
       " 'Ġange',\n",
       " 'ä¸ĭéĿ¢',\n",
       " 'Ġì¢',\n",
       " 'aczy',\n",
       " 'estab',\n",
       " 'Ġpersonaje',\n",
       " 'ÐºÐ°Ñħ',\n",
       " 'esus',\n",
       " '×ķ×ĵ×Ķ',\n",
       " 'ĠJimin',\n",
       " 'Ġwides',\n",
       " 'ĠëĤĺìĺ¤',\n",
       " 'çµ±',\n",
       " 'ãģ¹',\n",
       " 'ĠerzÃ¤h',\n",
       " 'astes',\n",
       " 'ĠìĺĪìģ',\n",
       " 'ä»Ĭ',\n",
       " 'ĠmÃ¶j',\n",
       " 'ĠjÄĻzy',\n",
       " 'Ġalg',\n",
       " 'ıĦë¡Ŀ',\n",
       " 'Ġdesem',\n",
       " 'EER',\n",
       " 'uestos',\n",
       " 'Ġìłķë§Ĳ',\n",
       " 'ĠëĦĺ',\n",
       " 'ìĶ¬',\n",
       " 'íĶ¼',\n",
       " 'ĠforcÃ©ment',\n",
       " 'ĠÃ©s',\n",
       " 'Ġmanera',\n",
       " 'ÑĢÐ°Ð¼',\n",
       " 'Ġbrac',\n",
       " 'Ġê·¸ëķĮ',\n",
       " 'Ġglaze',\n",
       " 'Ġalgum',\n",
       " 'Ġhaters',\n",
       " 'ĠÑĤÐ¾Ð½',\n",
       " 'Ġavanz',\n",
       " 'ĠìĿ¼ìĿ´',\n",
       " 'ĠKENNETH',\n",
       " 'Ġtutto',\n",
       " 'Ġstuffs',\n",
       " 'Ġallerdings',\n",
       " 'Ġvu',\n",
       " 'ÉĻr',\n",
       " 'ustering',\n",
       " 'ãħ',\n",
       " 'ĠÐ¿Ð¾Ñģ',\n",
       " 'ÑĸÑĤ',\n",
       " 'ĠBiz',\n",
       " 'Ġmetall',\n",
       " 'ĠÐ¼Ð°Ð»',\n",
       " 'íĽĦ',\n",
       " 'Ġgente',\n",
       " 'ieli',\n",
       " 'ĠÃ©po',\n",
       " 'Ġpodr',\n",
       " 'Ġì¢ĭìķĦ',\n",
       " 'Ġvoud',\n",
       " 'eft',\n",
       " 'Ġtiene',\n",
       " 'çµĲæŀľ',\n",
       " 'å®®',\n",
       " 'Ġaquest',\n",
       " 'Ġrealidad',\n",
       " 'Ġjum',\n",
       " 'ĠÐºÐ¾Ð½ÐµÑĩÐ½Ð¾',\n",
       " 'ĠÐ½Ð°Ð´Ð¾',\n",
       " 'ramatic',\n",
       " 'quar',\n",
       " 'ĠÑĥÐ¿',\n",
       " 'Ġwok',\n",
       " 'ĠÐ¢ÐµÐ¿ÐµÑĢÑĮ',\n",
       " 'Ġeins',\n",
       " 'Ġsiempre',\n",
       " 'Ġzijn',\n",
       " 'ĠSono',\n",
       " 'ĠverÃ¤ndert',\n",
       " 'Ġirr',\n",
       " 'Ġcalend',\n",
       " 'Ġjeszcze',\n",
       " 'Ġander',\n",
       " 'ĠÐ¿Ð¾Ð±ÐµÐ´',\n",
       " 'Ø§ÙĦÙħ',\n",
       " 'Ġolmas',\n",
       " 'rico',\n",
       " 'ĠÐ»ÑĥÑĩÑĪÐµ',\n",
       " 'å®¹',\n",
       " 'ĠpossÃŃvel',\n",
       " 'ĠcuÃ¡l',\n",
       " 'ĠÐ¿Ð¾Ð¿ÑĢÐ¾Ð±',\n",
       " 'Ð¸Ð¶Ñĥ',\n",
       " 'ĠYJ',\n",
       " 'ìĽĮ',\n",
       " 'inaÃ§Ã£o',\n",
       " 'Ð½Ð¾Ð³Ð¾',\n",
       " 'ĠëģĿ',\n",
       " 'Ġseria',\n",
       " 'ruk',\n",
       " 'Ã¼cht',\n",
       " 'ĠÐ¼Ð°ÑĢ',\n",
       " 'ĠcÃ´',\n",
       " 'ì²',\n",
       " '×ķ×Ĳ',\n",
       " 'Ġhade',\n",
       " 'Ġhiss',\n",
       " 'èĪĴ',\n",
       " 'ĠAunque',\n",
       " 'Ġê´ľì°®ìķĦ',\n",
       " 'Ġì¡°',\n",
       " 'Ġuk',\n",
       " 'ĠzurÃ¼ck',\n",
       " 'Ġviscosity',\n",
       " 'Ġavait',\n",
       " 'ĠnÃ©cess',\n",
       " 'ĠnegÃ³cio',\n",
       " 'ĠÐ´ÐµÐ¹ÑģÑĤÐ²',\n",
       " 'Äħt',\n",
       " 'Ġëĳ',\n",
       " 'ĠÐ¿Ð¾ÐºÑĥÐ¿',\n",
       " 'Ġcomplet',\n",
       " 'Ġbirthdays',\n",
       " 'avors',\n",
       " 'Ġraj',\n",
       " 'Ġbaix',\n",
       " 'Ġgemeinsam',\n",
       " 'Ġëĭ¹ìĭł',\n",
       " 'ĠMUR',\n",
       " 'Ġlekker',\n",
       " 'ĠgÃ¼',\n",
       " 'ĠJUDY',\n",
       " 'Ġlogar',\n",
       " 'ĠfÃ¼hrt',\n",
       " 'ĠmaÃ±ana',\n",
       " 'Ġnied',\n",
       " 'cuz',\n",
       " 'Ġprix',\n",
       " 'ĠÐ¾ÑĤÐ´ÐµÐ»ÑĮ',\n",
       " 'Ġabajo',\n",
       " 'Ġìķ½ê°Ħ',\n",
       " 'Ġmoonlight',\n",
       " 'èĽ',\n",
       " 'Ð½Ð¸ÑĤÑĮ',\n",
       " 'Ġhumano',\n",
       " 'ÐµÐºÑģÑĤ',\n",
       " 'niejsze',\n",
       " 'ìŀĸìķĦìļĶ',\n",
       " 'ÐµÐ»Ð¾Ð²',\n",
       " 'Ġcitoy',\n",
       " 'ĠÐ¿Ð¸Ñģ',\n",
       " 'Ġluz',\n",
       " 'éĹ®é¢ĺ',\n",
       " 'Ġpizzas',\n",
       " 'zub',\n",
       " 'Ġpouquinho',\n",
       " 'åĲ§',\n",
       " 'Ġvenge',\n",
       " 'ĠSny',\n",
       " 'Ġviendo',\n",
       " 'Ġconsomm',\n",
       " 'é¦Ļ',\n",
       " 'ÐºÐ»ÑİÑĩ',\n",
       " 'Ġcamb',\n",
       " 'ĠÐµÐ³Ð¾',\n",
       " 'ĠÐ¼Ð½Ð¾Ð¹',\n",
       " 'Ġmondo',\n",
       " 'ÑĤÐµÑĢ',\n",
       " 'ĠParlament',\n",
       " 'Ġdiyorsun',\n",
       " 'ÑĸÐ»ÑĮ',\n",
       " 'ãħł',\n",
       " 'ÑĩÐµ',\n",
       " 'Ġsimplemente',\n",
       " 'ÄĻ',\n",
       " 'ĠannÃ©e',\n",
       " 'Ġsera',\n",
       " 'íĮĿ',\n",
       " 'ĠëĶ',\n",
       " 'Ġsach',\n",
       " 'Ġwszystk',\n",
       " 'ĠÐ²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑģÑĤÑĮ',\n",
       " 'zeni',\n",
       " 'ìª½',\n",
       " 'ĠØ§ÙĦÙĬ',\n",
       " 'Ð¸Ñħ',\n",
       " 'peut',\n",
       " 'Ġà²',\n",
       " 'ĠÐ±Ð»',\n",
       " 'Ġaquesta',\n",
       " 'Ġpyram',\n",
       " 'Ã¹',\n",
       " '´ìŀ¥',\n",
       " 'gangen',\n",
       " 'Ġswo',\n",
       " 'ìĪ',\n",
       " 'Ġjakie',\n",
       " '´ëıĦ',\n",
       " 'ĠNove',\n",
       " 'Ġpronto',\n",
       " 'gend',\n",
       " 'Ġbers',\n",
       " 'ĠBueno',\n",
       " 'åĳ¨',\n",
       " 'ìķĺëĬĶëį°',\n",
       " 'Ġimpos',\n",
       " 'ĠìĺĪìģĺ',\n",
       " 'Ġê°Ģì§Ģ',\n",
       " 'Ġë¨¹ìĿĦ',\n",
       " 'ìłľë¡ľ',\n",
       " 'Ġ×Ķ×',\n",
       " 'ĠÑįÐº',\n",
       " 'ĠpiÅŁ',\n",
       " 'Ġsociedad',\n",
       " 'ìĸ´ìļĶ',\n",
       " 'Ġabre',\n",
       " 'Ġjag',\n",
       " 'Ġlorsque',\n",
       " 'olut',\n",
       " 'Ġë©',\n",
       " 'Ġterminar',\n",
       " 'BRUNO',\n",
       " 'Ġë§Ŀ',\n",
       " 'Ġud',\n",
       " 'Ġaproxim',\n",
       " 'Ġmote',\n",
       " 'Ġimmort',\n",
       " 'emente',\n",
       " 'ĠGÃ¼',\n",
       " 'ĠãĢĲ',\n",
       " 'ordo',\n",
       " 'usement',\n",
       " 'eler',\n",
       " 'Ġanci',\n",
       " 'ĠíķŃ',\n",
       " 'ĠâĢĳâĢĳ',\n",
       " 'Ġandar',\n",
       " 'Ġrecipro',\n",
       " 'ÑģÑĤÐ²Ð¸Ðµ',\n",
       " 'åħ¨éĥ¨',\n",
       " 'ĠëĦ£',\n",
       " 'Æ°ng',\n",
       " 'rÃ¥n',\n",
       " 'ì²ĺë',\n",
       " 'izar',\n",
       " 'ĠÐ¿Ð°ÑĢÑĥ',\n",
       " 'ĠHazrat',\n",
       " 'jae',\n",
       " 'Ġlegg',\n",
       " 'ãĤĤãģĨ',\n",
       " 'ĠsiÃ¨',\n",
       " 'Ġkab',\n",
       " 'ĠÐ´Ð¾Ð³',\n",
       " 'ĠBeni',\n",
       " 'ä¸ĸ',\n",
       " 'ĠAudience',\n",
       " 'ÐµÐ½Ð½Ð¾Ð³Ð¾',\n",
       " 'odu',\n",
       " 'å¿µ',\n",
       " 'Ġgrille',\n",
       " 'Ú©',\n",
       " 'ĠëĤĺëıĦ',\n",
       " 'Ġê°Ģê¹Į',\n",
       " 'zysta',\n",
       " 'åģļ',\n",
       " 'Ġbrasile',\n",
       " 'ĠJIM',\n",
       " 'çĿĢ',\n",
       " 'Ġistedi',\n",
       " 'æĿ¾',\n",
       " 'ĠkÃ¤',\n",
       " 'ĠÑĤÐ°Ð½',\n",
       " 'å°įåķĬ',\n",
       " 'Ġè¬Ŀè¬Ŀ',\n",
       " 'ĠSPEAK',\n",
       " 'ëĬĺ',\n",
       " 'hehe',\n",
       " 'ĠëĮĵ',\n",
       " 'çĻ½',\n",
       " 'ĠMonst',\n",
       " 'Ġëı',\n",
       " 'ĠÐ¸Ð·Ð²',\n",
       " 'Ġtoolkit',\n",
       " 'usic',\n",
       " 'ç«Ļ',\n",
       " 'è¸',\n",
       " 'Ã¤tte',\n",
       " 'Ġëª°ë',\n",
       " 'Ġinteressante',\n",
       " 'utet',\n",
       " 'ìĥĿ',\n",
       " 'ÏĥÎ·',\n",
       " 'å¾®',\n",
       " 'ĠÑĩÑĥÐ²',\n",
       " 'ì¤ĳìĹĲ',\n",
       " 'ĠManh',\n",
       " 'iemand',\n",
       " 'Ġcomo',\n",
       " 'ĠÐ²Ð¸Ð´Ð¸ÑĤÐµ',\n",
       " 'gic',\n",
       " 'ĠÑĢÐ°Ð·Ð½ÑĭÐµ',\n",
       " 'Ġrecher',\n",
       " 'Ġbanc',\n",
       " 'ĠVAN',\n",
       " 'kum',\n",
       " 'tawa',\n",
       " 'ĠdemandÃ©',\n",
       " 'Ġwszystko',\n",
       " '³´ëĭ¤',\n",
       " 'esser',\n",
       " 'ĠÙĨÛģ',\n",
       " 'Ġverbess',\n",
       " 'ĠÑĹ',\n",
       " 'ĠÐ¾ÑĤ',\n",
       " 'ĠÐĽÑİ',\n",
       " 'ĠIst',\n",
       " 'imas',\n",
       " 'Ġew',\n",
       " 'Ġìĸ´ëĶ',\n",
       " 'Ġredo',\n",
       " 'Ġbardziej',\n",
       " 'Ġbeige',\n",
       " 'Ġunknowns',\n",
       " 'Ġpergunta',\n",
       " 'Ġfelic',\n",
       " 'Ġroasting',\n",
       " 'ç½',\n",
       " 'Ġrans',\n",
       " 'Ġë°¥',\n",
       " 'Ã¤lle',\n",
       " 'ĠíĻĺ',\n",
       " 'ĠPÃ¥',\n",
       " 'ç´ħ',\n",
       " 'Ġfigura',\n",
       " 'ĠWeil',\n",
       " 'Ġë¨¹ê³ł',\n",
       " 'ĠDAVID',\n",
       " 'iggling',\n",
       " 'Ã²n',\n",
       " 'Ġroommates',\n",
       " 'Æ°á»Ŀi',\n",
       " 'ĠkÃ¤ytt',\n",
       " 'Ġepisod',\n",
       " 'ĠÐ·Ð²ÑĥÑĩ',\n",
       " 'nehmen',\n",
       " 'welling',\n",
       " 'ĠElo',\n",
       " 'Ġquesta',\n",
       " 'Ġalgo',\n",
       " 'ÙħØ©',\n",
       " 'ĠczÅĤowie',\n",
       " 'Ġelimin',\n",
       " 'èģ½',\n",
       " 'Ð¿ÑĢÐ¸',\n",
       " 'Ġnovamente',\n",
       " 'ÙĬÙħ',\n",
       " 'å½¢',\n",
       " 'ĠëŃĲìķ¼',\n",
       " 'Ġë§ĽìŀĪëĬĶ',\n",
       " 'Ġíķ´ìļĶ',\n",
       " 'Ġerf',\n",
       " 'Ġstrony',\n",
       " 'ĠìĤ´ë',\n",
       " 'crowd',\n",
       " 'ĠFER',\n",
       " 'Ġãģĵ',\n",
       " 'ç©º',\n",
       " 'chs',\n",
       " 'ĠÑĢÐ°Ð½',\n",
       " '.âĢĭ',\n",
       " 'Ġzobaczy',\n",
       " 'RISADAS',\n",
       " 'aÃ§Ã£o',\n",
       " 'ÑĥÐ½Ð´',\n",
       " 'ÑģÐ¾Ð½',\n",
       " 'å®¢',\n",
       " 'ØµÙĦ',\n",
       " 'ÄŁÄ±',\n",
       " 'ÙĩØ§',\n",
       " 'ĠvÃŃ',\n",
       " 'åĪ¥',\n",
       " 'ÏĥÏĦÎµ',\n",
       " 'Ġsweetheart',\n",
       " 'ĠÎ®',\n",
       " 'Ġvara',\n",
       " 'é¼',\n",
       " 'çĶľ',\n",
       " 'Ġeinfach',\n",
       " 'ĠìķĦëĭĪëĿ¼',\n",
       " 'å¦Ĥæŀľ',\n",
       " 'Ġìĭ¶',\n",
       " 'ìĸ',\n",
       " 'ĠMEL',\n",
       " 'ÑĪ',\n",
       " 'ogr',\n",
       " 'éľ',\n",
       " 'Ġentre',\n",
       " 'Ġtrainee',\n",
       " 'ĠSheng',\n",
       " 'ìłķ',\n",
       " 'fry',\n",
       " 'áº¥',\n",
       " 'Ġdeuts',\n",
       " 'ì¹ĺë¥¼',\n",
       " 'Ġentreprises',\n",
       " 'Ġpunkt',\n",
       " 'Ġmuá»ĳn',\n",
       " 'ì¹¨',\n",
       " 'ĠÑĤÐµÑĢ',\n",
       " 'ĠmÃªs',\n",
       " 'ĠCin',\n",
       " 'Ġmomentos',\n",
       " '°ĺ',\n",
       " 'ØªÙĬ',\n",
       " 'ĠErde',\n",
       " 'ĠArsen',\n",
       " 'ĠëĤĺëĪ',\n",
       " 'enseful',\n",
       " 'Ã®t',\n",
       " 'Ð¸Ð²Ð°ÑİÑĤ',\n",
       " 'Ð½ÐµÐµ',\n",
       " 'ordum',\n",
       " 'Ġpong',\n",
       " 'ĠÑģÐ¾Ð±',\n",
       " 'Ġhistoire',\n",
       " 'ĠGoodness',\n",
       " 'ìĺ¨',\n",
       " 'Ġmicrof',\n",
       " 'izaciÃ³n',\n",
       " 'voll',\n",
       " 'ĠÐ²ÑģÐµÐ³Ð´Ð°',\n",
       " 'ĠÐ¼Ð¾Ð¶ÐµÐ¼',\n",
       " 'Ġtuvo',\n",
       " 'ĠktÃ³',\n",
       " 'ĠaÃ±ad',\n",
       " 'waÅ¼',\n",
       " 'Ġcoloss',\n",
       " 'Ġë§Īë',\n",
       " 'ĠJÃ¡',\n",
       " 'Ġznaj',\n",
       " 'forder',\n",
       " 'ĠDIRE',\n",
       " 'ĠÐ¼Ð¾Ð´',\n",
       " 'ØŃ',\n",
       " 'ĠëĲĺìĸ´',\n",
       " 'ieten',\n",
       " 'erecht',\n",
       " 'ä¹Ĳ',\n",
       " 'éĢı',\n",
       " 'ĠÐ¿Ð¾Ð»Ð½Ð¾ÑģÑĤÑĮÑİ',\n",
       " '§¤',\n",
       " 'Ġpastel',\n",
       " 'Ġphi',\n",
       " 'Ð½Ð¸ÑĤÐµ',\n",
       " 'Ġmodelo',\n",
       " 'çĶŁæ´»',\n",
       " 'èģŀ',\n",
       " 'ç¯ĢçĽ®',\n",
       " 'Ġcommen',\n",
       " 'ĠÐ½Ð°Ð²ÐµÑĢ',\n",
       " 'ĠCuando',\n",
       " 'æ¬',\n",
       " 'Ġtehd',\n",
       " 'cÄ±k',\n",
       " 'Ġkein',\n",
       " 'vem',\n",
       " 'ĠAntes',\n",
       " 'ombres',\n",
       " 'Ġese',\n",
       " 'ĠÐĶ',\n",
       " 'buat',\n",
       " 'å®³',\n",
       " 'Įë',\n",
       " 'Ġkommer',\n",
       " 'åĭķçĶ»',\n",
       " 'Ġpasso',\n",
       " 'Ġschlim',\n",
       " 'ĠChanel',\n",
       " 'ulen',\n",
       " 'Ġnyt',\n",
       " 'Ġì§Ħ',\n",
       " 'ĠìŀĪëĭ¤ê³ł',\n",
       " 'Ġmue',\n",
       " 'åħĪ',\n",
       " 'ì§Ģê°Ģ',\n",
       " 'Ä±sÄ±',\n",
       " 'RIA',\n",
       " 'ĠÐ¿Ð¾Ð¼',\n",
       " 'åİ»',\n",
       " 'Ġmotherf',\n",
       " 'ugo',\n",
       " 'ĠÐ´Ð¾Ð¿',\n",
       " 'fon',\n",
       " 'Ð°Ð»ÑĮÐ½ÑĭÑħ',\n",
       " 'Ġdevient',\n",
       " 'ĠÐ¼Ð¾Ð¶',\n",
       " 'åľŁ',\n",
       " 'Ġplataforma',\n",
       " 'Ġonun',\n",
       " 'Ġbli',\n",
       " 'Ġpanda',\n",
       " 'Ġdost',\n",
       " 'gunta',\n",
       " 'Ġanders',\n",
       " 'istles',\n",
       " 'Ġzawsze',\n",
       " 'olÃ©',\n",
       " 'Ġä¸įæĺ¯',\n",
       " 'Ġmemorized',\n",
       " 'insp',\n",
       " 'ĠÐ¼ÐµÐ»',\n",
       " 'ãĤįãģĨ',\n",
       " 'wurf',\n",
       " 'ĠÏĦÎ¿Î½',\n",
       " 'zk',\n",
       " 'à¹Ģà¸ŀ',\n",
       " 'ÑģÐ»ÐµÐ´',\n",
       " 'Ġkamera',\n",
       " 'Ġ×Ķ×§',\n",
       " 'ĠÎ¼Î¿Ïħ',\n",
       " 'Ġconocer',\n",
       " 'ĠÐ¼Ð¸ÑĢ',\n",
       " 'issent',\n",
       " 'ĠÐ·Ð°Ð±',\n",
       " 'ĠìķĬê³ł',\n",
       " 'ĠcrianÃ§as',\n",
       " 'Ġexiste',\n",
       " 'ĠìķĪìĹĲ',\n",
       " 'ĠÐ²ÑĭÐ¿',\n",
       " 'Ġë²Īì§¸',\n",
       " 'Ġmaneira',\n",
       " 'Ġtemas',\n",
       " 'uego',\n",
       " 'Ð¸ÑĩÐµÑģÐºÐ¸Ðµ',\n",
       " 'ĠKampf',\n",
       " 'Ã§o',\n",
       " 'phants',\n",
       " 'Ġgour',\n",
       " 'Ġchoreography',\n",
       " 'Ġmedios',\n",
       " 'è³½',\n",
       " 'Ð»Ð¸Ð½',\n",
       " 'ieben',\n",
       " 'Ġsof',\n",
       " 'Ġvoltar',\n",
       " 'ĠÐ³',\n",
       " 'ĠPicasso',\n",
       " 'Ġë»',\n",
       " 'Ġãħĭãħĭãħĭãħĭ',\n",
       " 'ì¢ħ',\n",
       " 'Ġíľ',\n",
       " 'Ġrien',\n",
       " 'Ġescuela',\n",
       " 'Ġíķ',\n",
       " 'ĠÐ¾ÐºÐ°Ð·',\n",
       " 'Ġstanie',\n",
       " '×ĳ×',\n",
       " 'Ġë¬',\n",
       " 'ĠìłĲ',\n",
       " 'ĠHJ',\n",
       " 'ĠAdemÃ¡s',\n",
       " 'Ġfas',\n",
       " 'ĠØ£ÙĨØ§',\n",
       " 'Ġdrawers',\n",
       " 'Ġsemanas',\n",
       " 'Ġbeispielsweise',\n",
       " 'ĠfÃ¥',\n",
       " 'bild',\n",
       " 'Ġonu',\n",
       " 'ç²¾',\n",
       " 'encias',\n",
       " 'á»Ľi',\n",
       " 'gestellt',\n",
       " 'ç²',\n",
       " '¶Ģ',\n",
       " 'Ð°Ð½Ð¸Ñı',\n",
       " 'Ġaurait',\n",
       " 'ĠNicki',\n",
       " 'Ġì²«',\n",
       " 'Ð»Ð¾Ñħ',\n",
       " 'aÄĩ',\n",
       " 'Ġavoir',\n",
       " 'åĵ¡',\n",
       " 'ĠÑĢÐµÑĪÐ¸Ð»',\n",
       " 'Ø´',\n",
       " 'Ġconduction',\n",
       " 'ĠâĻª',\n",
       " 'Ġzat',\n",
       " 'ÏģÎ·',\n",
       " 'ĠÐĲÐ»ÐµÐºÑģÐ°Ð½Ð´',\n",
       " 'ĠÐ¸Ð½Ð´',\n",
       " 'ÐºÐ¾Ð»ÑĮ',\n",
       " 'ãĤĦãģ£ãģ¦',\n",
       " 'Hola',\n",
       " '?âĢĭ',\n",
       " 'Ġmellan',\n",
       " 'ĠktÃ³rej',\n",
       " 'Ġcambio',\n",
       " 'Ġvuel',\n",
       " 'Ġclic',\n",
       " 'Ġë§Ĳ',\n",
       " 'Ãªncia',\n",
       " 'Ġcrucified',\n",
       " 'Ġoverst',\n",
       " 'ãĢħ',\n",
       " 'Ġtuck',\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tokenizer1.vocab.keys()) - set(tokenizer2.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20877"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokenizer2.vocab.keys()) - set(tokenizer1.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20878"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokenizer2.vocab.keys()) - set(tokenizer4.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51864"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer2.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29992"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer3.vocab['xi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
