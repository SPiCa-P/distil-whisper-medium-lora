{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperTokenizerFast, WhisperFeatureExtractor,get_scheduler,WhisperProcessor\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "import datasets\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer, EnglishTextNormalizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25d170ac4214e54b5a191b9760ad46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ce28e974494fe391c82fd9db1ffbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54e64fcf6084d46836519c8e0619775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds1 = load_dataset('facebook/multilingual_librispeech',\n",
    "                    'german',\n",
    "                    cache_dir=\"/media/hdd/.cache/huggingface\",\n",
    "            )\n",
    "\n",
    "ds2 = load_dataset('mozilla-foundation/common_voice_16_0',\n",
    "                    'de',\n",
    "                    cache_dir=\"/media/hdd_old/.cache/huggingface\",\n",
    "                )\n",
    "\n",
    "ds3 = load_dataset(\n",
    "            'facebook/voxpopuli',\n",
    "            'de',\n",
    "            cache_dir=\"/media/hdd_old/.cache/huggingface\",\n",
    "            )\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"distil-whisper/distil-medium.en\")\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(\"distil-whisper/distil-medium.en\")\n",
    "\n",
    "def rename_columns(ds, column_nammes):\n",
    "    ds = ds.cast_column(\"audio\", datasets.features.Audio(16000))\n",
    "\n",
    "    ds = ds.rename_column(column_nammes, \"text\")\n",
    "    \n",
    "    dataset_features = ds['train'].features.keys()\n",
    "    columns_to_keep = {\"audio\", \"text\"}\n",
    "    ds = ds.remove_columns(set(dataset_features - columns_to_keep))\n",
    "    \n",
    "    return ds\n",
    "    \n",
    "ds1 = rename_columns(ds1, \"transcript\")\n",
    "ds2 = rename_columns(ds2, \"sentence\")\n",
    "ds3 = rename_columns(ds3, \"raw_text\")\n",
    "\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = concatenate_datasets([ds1['train'], ds2['train'], ds2['validation']])\n",
    "dataset['ID_eval'] = concatenate_datasets([ds1['test'], ds2['test']])\n",
    "dataset['OOD_eval'] = concatenate_datasets([ds3['validation'], ds3['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'] = dataset['train'].shuffle(seed=41).take(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"Function {func.__name__} took {end_time - start_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['audio', 'text']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets_train_features = list(dataset[\"train\"].features.keys())\n",
    "raw_datasets_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def prepare_train_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = [sample[\"array\"] for sample in batch[\"audio\"]]\n",
    "    inputs = feature_extractor(audio, sampling_rate=16000, device='cuda')\n",
    "    batch[\"input_features\"] = inputs.input_features\n",
    "    batch[\"input_length\"] = [len(sample) for sample in audio]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[300, 800, 100, 500]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "list = [10, 100, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bd2a323cce43fd90e9e8a78ae5b225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function prepare_train_dataset took 5.2748 seconds\n",
      "Function prepare_train_dataset took 5.0388 seconds\n",
      "Function prepare_train_dataset took 5.1649 seconds\n",
      "Function prepare_train_dataset took 1.7884 seconds\n",
      "batch_size: 300, time: 117.6985 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fa53a4f7c64994a06e7a9af1d75f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function prepare_train_dataset took 14.6061 seconds\n",
      "Function prepare_train_dataset took 3.4520 seconds\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/hdd/.cache/huggingface/facebook___multilingual_librispeech/german/0.0.0/2e83e61823b4c47dcbcb1980bb88601274127609/tmp7wp0rd39'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/shutil.py:886\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 886\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/hdd/.cache/huggingface/facebook___multilingual_librispeech/german/0.0.0/2e83e61823b4c47dcbcb1980bb88601274127609/tmp7wp0rd39' -> '/media/hdd/.cache/huggingface/facebook___multilingual_librispeech/german/0.0.0/2e83e61823b4c47dcbcb1980bb88601274127609/cache-d4c5902c7013d02e.arrow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m      3\u001b[0m     tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare_train_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m tic\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3167\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3162\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3163\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3164\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3165\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3166\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3167\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3168\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3603\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_data \u001b[38;5;129;01mand\u001b[39;00m tmp_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3602\u001b[0m     tmp_file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 3603\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3604\u001b[0m     umask \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mumask(\u001b[38;5;241m0o666\u001b[39m)\n\u001b[1;32m   3605\u001b[0m     os\u001b[38;5;241m.\u001b[39mumask(umask)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/shutil.py:906\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    904\u001b[0m         rmtree(src)\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 906\u001b[0m         \u001b[43mcopy_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m         os\u001b[38;5;241m.\u001b[39munlink(src)\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m real_dst\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/shutil.py:475\u001b[0m, in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    473\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m copystat(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/shutil.py:260\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    258\u001b[0m     os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mreadlink(src), dst)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;66;03m# macOS\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/hdd/.cache/huggingface/facebook___multilingual_librispeech/german/0.0.0/2e83e61823b4c47dcbcb1980bb88601274127609/tmp7wp0rd39'"
     ]
    }
   ],
   "source": [
    "for batch_size in list:\n",
    "    \n",
    "    tic = time.time()\n",
    "    dataset['train'].map(prepare_train_dataset, batched=True, batch_size=batch_size)\n",
    "    toc = time.time() - tic\n",
    "    \n",
    "    print(f\"batch_size: {batch_size}, time: {toc:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3333333333'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"3\" * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'30303030303030303030'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'30' * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"3\" * 10 > '30' * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf7523f2fcd404c9ed9bda259e787ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0029 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0053 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0030 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0030 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0030 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0031 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0031 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0052 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0057 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0055 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0067 seconds\n",
      "Function prepare_train_dataset took 0.0065 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0060 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0053 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0029 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0054 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0061 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0053 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0054 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0058 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0056 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0053 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0061 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0052 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0054 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0061 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0033 seconds\n",
      "Function prepare_train_dataset took 0.0052 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0031 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0057 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0053 seconds\n",
      "Function prepare_train_dataset took 0.0054 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0030 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0051 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0048 seconds\n",
      "Function prepare_train_dataset took 0.0034 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0041 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0050 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0032 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0072 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0040 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n",
      "Function prepare_train_dataset took 0.0052 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0037 seconds\n",
      "Function prepare_train_dataset took 0.0036 seconds\n",
      "Function prepare_train_dataset took 0.0045 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0043 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0046 seconds\n",
      "Function prepare_train_dataset took 0.0044 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0039 seconds\n",
      "Function prepare_train_dataset took 0.0049 seconds\n",
      "Function prepare_train_dataset took 0.0047 seconds\n",
      "Function prepare_train_dataset took 0.0035 seconds\n",
      "Function prepare_train_dataset took 0.0042 seconds\n",
      "Function prepare_train_dataset took 0.0038 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset['train'] = dataset['train'].map(prepare_train_dataset, batched=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17.19,\n",
       " 17.62,\n",
       " 10.991125,\n",
       " 18.8,\n",
       " 16.15,\n",
       " 15.87,\n",
       " 18.37,\n",
       " 14.54,\n",
       " 18.37,\n",
       " 18.95,\n",
       " 15.21,\n",
       " 17.57,\n",
       " 14.49,\n",
       " 16.92,\n",
       " 13.85,\n",
       " 11.12,\n",
       " 12.98,\n",
       " 11.38,\n",
       " 15.19,\n",
       " 12.74,\n",
       " 12.2,\n",
       " 13.63,\n",
       " 16.22,\n",
       " 18.06,\n",
       " 14.52,\n",
       " 17.82,\n",
       " 13.53,\n",
       " 11.66,\n",
       " 14.5,\n",
       " 14.67,\n",
       " 12.53,\n",
       " 13.24,\n",
       " 16.36,\n",
       " 15.68,\n",
       " 13.82,\n",
       " 12.54,\n",
       " 10.61,\n",
       " 17.68,\n",
       " 10.14,\n",
       " 16.68,\n",
       " 10.5,\n",
       " 12.3,\n",
       " 10.27,\n",
       " 13.81,\n",
       " 12.13,\n",
       " 10.29,\n",
       " 12.56,\n",
       " 16.62,\n",
       " 20.0,\n",
       " 12.05,\n",
       " 13.02,\n",
       " 14.53,\n",
       " 15.95,\n",
       " 11.4,\n",
       " 16.72,\n",
       " 14.62,\n",
       " 17.03,\n",
       " 12.86,\n",
       " 12.93,\n",
       " 19.98,\n",
       " 17.16,\n",
       " 14.61,\n",
       " 16.54,\n",
       " 16.91,\n",
       " 16.92,\n",
       " 19.2,\n",
       " 10.75,\n",
       " 14.71,\n",
       " 13.33,\n",
       " 17.99,\n",
       " 10.63,\n",
       " 18.62,\n",
       " 12.17,\n",
       " 10.23,\n",
       " 18.46,\n",
       " 10.72,\n",
       " 10.94,\n",
       " 14.29,\n",
       " 13.29,\n",
       " 11.62,\n",
       " 17.53,\n",
       " 10.72,\n",
       " 13.64,\n",
       " 14.96,\n",
       " 11.89,\n",
       " 17.71,\n",
       " 11.8,\n",
       " 14.87,\n",
       " 16.49,\n",
       " 12.36,\n",
       " 11.12,\n",
       " 14.54,\n",
       " 14.94,\n",
       " 13.21,\n",
       " 15.64,\n",
       " 14.11,\n",
       " 12.96,\n",
       " 17.94,\n",
       " 11.17,\n",
       " 19.72,\n",
       " 13.83,\n",
       " 10.52,\n",
       " 16.61,\n",
       " 12.58,\n",
       " 12.21,\n",
       " 16.04,\n",
       " 11.41,\n",
       " 20.0,\n",
       " 12.84,\n",
       " 11.73,\n",
       " 17.89,\n",
       " 11.01,\n",
       " 10.04,\n",
       " 11.48,\n",
       " 14.68,\n",
       " 11.57,\n",
       " 18.5,\n",
       " 13.78,\n",
       " 12.77,\n",
       " 11.63,\n",
       " 17.86,\n",
       " 15.18,\n",
       " 12.55,\n",
       " 10.14,\n",
       " 18.9,\n",
       " 10.31,\n",
       " 13.75,\n",
       " 10.0,\n",
       " 15.04,\n",
       " 12.05,\n",
       " 18.47,\n",
       " 10.57,\n",
       " 18.46,\n",
       " 13.12,\n",
       " 14.76,\n",
       " 14.61,\n",
       " 15.7,\n",
       " 14.45,\n",
       " 13.48,\n",
       " 17.95,\n",
       " 13.53,\n",
       " 13.35,\n",
       " 13.29,\n",
       " 16.16,\n",
       " 11.68,\n",
       " 10.44,\n",
       " 13.99,\n",
       " 11.3,\n",
       " 10.88,\n",
       " 16.33,\n",
       " 17.18,\n",
       " 19.18,\n",
       " 14.08,\n",
       " 11.6,\n",
       " 14.08,\n",
       " 11.11,\n",
       " 12.87,\n",
       " 14.83,\n",
       " 13.68,\n",
       " 12.98,\n",
       " 10.69,\n",
       " 14.48,\n",
       " 13.71,\n",
       " 11.17,\n",
       " 10.9,\n",
       " 12.03,\n",
       " 17.32,\n",
       " 14.25,\n",
       " 19.61,\n",
       " 10.27,\n",
       " 17.78,\n",
       " 13.65,\n",
       " 11.96,\n",
       " 14.2,\n",
       " 13.82,\n",
       " 13.7,\n",
       " 14.95,\n",
       " 15.01,\n",
       " 12.17,\n",
       " 11.26,\n",
       " 11.87,\n",
       " 15.17,\n",
       " 11.25,\n",
       " 15.27,\n",
       " 10.74,\n",
       " 13.84,\n",
       " 17.29,\n",
       " 11.08,\n",
       " 17.61,\n",
       " 18.16,\n",
       " 18.99,\n",
       " 14.08,\n",
       " 18.93,\n",
       " 15.8,\n",
       " 17.19,\n",
       " 15.38,\n",
       " 15.24,\n",
       " 15.26,\n",
       " 12.28,\n",
       " 14.55,\n",
       " 13.6,\n",
       " 14.41,\n",
       " 12.4,\n",
       " 10.03,\n",
       " 12.73,\n",
       " 11.28,\n",
       " 18.57,\n",
       " 17.52,\n",
       " 11.45,\n",
       " 12.12,\n",
       " 10.62,\n",
       " 17.6,\n",
       " 10.07,\n",
       " 15.56,\n",
       " 17.77,\n",
       " 11.97,\n",
       " 17.06,\n",
       " 13.63,\n",
       " 11.34,\n",
       " 10.22,\n",
       " 16.38,\n",
       " 11.83,\n",
       " 17.53,\n",
       " 19.11,\n",
       " 16.76,\n",
       " 10.49,\n",
       " 17.17,\n",
       " 16.61,\n",
       " 14.69,\n",
       " 15.64,\n",
       " 19.02,\n",
       " 19.45,\n",
       " 11.11,\n",
       " 18.73,\n",
       " 12.87,\n",
       " 17.56,\n",
       " 19.44,\n",
       " 12.43,\n",
       " 18.81,\n",
       " 18.65,\n",
       " 12.5,\n",
       " 18.55,\n",
       " 11.28,\n",
       " 12.99,\n",
       " 13.0,\n",
       " 14.75,\n",
       " 18.28,\n",
       " 11.1,\n",
       " 18.17,\n",
       " 18.4,\n",
       " 15.61,\n",
       " 15.01,\n",
       " 15.18,\n",
       " 18.29,\n",
       " 16.8,\n",
       " 13.17,\n",
       " 17.91,\n",
       " 19.35,\n",
       " 14.41,\n",
       " 19.04,\n",
       " 13.86,\n",
       " 13.67,\n",
       " 13.92,\n",
       " 11.47,\n",
       " 19.52,\n",
       " 11.24,\n",
       " 17.28,\n",
       " 14.84,\n",
       " 17.51,\n",
       " 16.84,\n",
       " 12.98,\n",
       " 10.1,\n",
       " 16.66,\n",
       " 15.22,\n",
       " 19.48,\n",
       " 16.8,\n",
       " 13.45,\n",
       " 12.43,\n",
       " 14.28,\n",
       " 16.27,\n",
       " 13.2,\n",
       " 16.92,\n",
       " 10.95,\n",
       " 16.41,\n",
       " 12.82,\n",
       " 13.15,\n",
       " 14.44,\n",
       " 17.0,\n",
       " 14.73,\n",
       " 15.64,\n",
       " 15.16,\n",
       " 12.42,\n",
       " 12.51,\n",
       " 14.97,\n",
       " 10.26,\n",
       " 13.36,\n",
       " 12.17,\n",
       " 15.76,\n",
       " 13.32,\n",
       " 11.75,\n",
       " 10.56,\n",
       " 16.43,\n",
       " 13.33,\n",
       " 10.85,\n",
       " 10.65,\n",
       " 17.6,\n",
       " 12.56,\n",
       " 14.55,\n",
       " 14.23,\n",
       " 19.32,\n",
       " 12.77,\n",
       " 15.33,\n",
       " 13.92,\n",
       " 18.16,\n",
       " 12.18,\n",
       " 13.13,\n",
       " 11.69,\n",
       " 15.84,\n",
       " 11.9,\n",
       " 14.15,\n",
       " 14.01,\n",
       " 14.36,\n",
       " 15.67,\n",
       " 10.48,\n",
       " 19.48,\n",
       " 13.97,\n",
       " 14.94,\n",
       " 14.75,\n",
       " 16.07,\n",
       " 14.51,\n",
       " 14.47,\n",
       " 13.11,\n",
       " 15.61,\n",
       " 10.74,\n",
       " 19.45,\n",
       " 15.91,\n",
       " 14.26,\n",
       " 13.33,\n",
       " 11.58,\n",
       " 16.29,\n",
       " 16.33,\n",
       " 17.01,\n",
       " 14.29,\n",
       " 13.31,\n",
       " 11.27,\n",
       " 10.91,\n",
       " 19.23,\n",
       " 17.64,\n",
       " 10.69,\n",
       " 19.27,\n",
       " 15.26,\n",
       " 16.37,\n",
       " 18.56,\n",
       " 17.12,\n",
       " 11.22,\n",
       " 13.41,\n",
       " 16.75,\n",
       " 14.16,\n",
       " 14.67,\n",
       " 15.36,\n",
       " 16.231125,\n",
       " 12.7,\n",
       " 12.43,\n",
       " 11.07,\n",
       " 14.35,\n",
       " 19.37,\n",
       " 10.8,\n",
       " 16.94,\n",
       " 12.64,\n",
       " 16.87,\n",
       " 13.34,\n",
       " 16.83,\n",
       " 19.91,\n",
       " 15.03,\n",
       " 11.07,\n",
       " 12.08,\n",
       " 14.83,\n",
       " 18.01,\n",
       " 14.97,\n",
       " 11.6,\n",
       " 18.71,\n",
       " 13.68,\n",
       " 12.6,\n",
       " 12.49,\n",
       " 14.47,\n",
       " 12.01,\n",
       " 19.36,\n",
       " 17.38,\n",
       " 17.93,\n",
       " 17.26,\n",
       " 14.7,\n",
       " 12.02,\n",
       " 19.81,\n",
       " 13.71,\n",
       " 19.11,\n",
       " 13.76,\n",
       " 19.27,\n",
       " 12.31,\n",
       " 17.21,\n",
       " 11.37,\n",
       " 19.75,\n",
       " 15.22,\n",
       " 10.21,\n",
       " 10.46,\n",
       " 18.37,\n",
       " 12.02,\n",
       " 13.23,\n",
       " 17.54,\n",
       " 12.88,\n",
       " 15.38,\n",
       " 14.4,\n",
       " 18.82,\n",
       " 10.25,\n",
       " 13.7,\n",
       " 15.42,\n",
       " 13.91,\n",
       " 11.17,\n",
       " 11.05,\n",
       " 14.72,\n",
       " 13.06,\n",
       " 11.23,\n",
       " 11.6770625,\n",
       " 18.55,\n",
       " 15.89,\n",
       " 13.5,\n",
       " 15.07,\n",
       " 15.87,\n",
       " 11.74,\n",
       " 15.32,\n",
       " 17.03,\n",
       " 19.92,\n",
       " 10.03,\n",
       " 14.01,\n",
       " 12.76,\n",
       " 16.07,\n",
       " 14.17,\n",
       " 12.15,\n",
       " 15.67,\n",
       " 13.74,\n",
       " 11.45,\n",
       " 11.69,\n",
       " 12.89,\n",
       " 15.05,\n",
       " 14.55,\n",
       " 10.12,\n",
       " 18.22,\n",
       " 12.8,\n",
       " 19.96,\n",
       " 14.41,\n",
       " 16.93,\n",
       " 16.11,\n",
       " 12.51,\n",
       " 18.02,\n",
       " 18.78,\n",
       " 19.35,\n",
       " 19.47,\n",
       " 19.58,\n",
       " 14.03,\n",
       " 11.89,\n",
       " 14.7,\n",
       " 15.52,\n",
       " 19.79,\n",
       " 12.79,\n",
       " 11.61,\n",
       " 14.19,\n",
       " 10.01,\n",
       " 16.16,\n",
       " 14.26,\n",
       " 16.66,\n",
       " 14.41,\n",
       " 19.44,\n",
       " 14.79,\n",
       " 10.38,\n",
       " 17.84,\n",
       " 12.3,\n",
       " 10.08,\n",
       " 16.39,\n",
       " 16.61,\n",
       " 10.97,\n",
       " 14.16,\n",
       " 15.87,\n",
       " 15.95,\n",
       " 16.07,\n",
       " 16.91,\n",
       " 10.04,\n",
       " 15.82,\n",
       " 18.2581875,\n",
       " 19.65,\n",
       " 16.03,\n",
       " 14.65,\n",
       " 13.61,\n",
       " 16.01,\n",
       " 18.66,\n",
       " 12.24,\n",
       " 18.85,\n",
       " 16.65,\n",
       " 15.53,\n",
       " 11.15,\n",
       " 13.21,\n",
       " 19.45,\n",
       " 11.19,\n",
       " 13.76,\n",
       " 18.71,\n",
       " 14.77,\n",
       " 16.43,\n",
       " 11.75,\n",
       " 10.04,\n",
       " 17.38,\n",
       " 14.59,\n",
       " 16.95,\n",
       " 15.28,\n",
       " 15.67,\n",
       " 10.84,\n",
       " 12.77,\n",
       " 17.73,\n",
       " 16.62,\n",
       " 10.73,\n",
       " 16.44,\n",
       " 13.54,\n",
       " 19.12,\n",
       " 15.42,\n",
       " 13.62,\n",
       " 18.68,\n",
       " 12.69,\n",
       " 15.44,\n",
       " 17.07,\n",
       " 16.72,\n",
       " 10.58,\n",
       " 18.54,\n",
       " 19.88,\n",
       " 10.93,\n",
       " 14.98,\n",
       " 17.1,\n",
       " 11.38,\n",
       " 14.71,\n",
       " 17.85,\n",
       " 14.92,\n",
       " 15.01,\n",
       " 17.71,\n",
       " 14.78,\n",
       " 17.72,\n",
       " 15.13,\n",
       " 13.44,\n",
       " 18.36,\n",
       " 19.19,\n",
       " 17.81,\n",
       " 18.36,\n",
       " 17.17,\n",
       " 16.9,\n",
       " 19.16,\n",
       " 15.85,\n",
       " 11.71,\n",
       " 15.77,\n",
       " 12.68,\n",
       " 13.96,\n",
       " 19.46,\n",
       " 16.01,\n",
       " 11.37,\n",
       " 15.15,\n",
       " 11.97,\n",
       " 13.37,\n",
       " 19.98,\n",
       " 15.95,\n",
       " 12.11,\n",
       " 10.24,\n",
       " 10.89,\n",
       " 11.64,\n",
       " 12.76,\n",
       " 19.96,\n",
       " 16.58,\n",
       " 11.61,\n",
       " 14.24,\n",
       " 18.04,\n",
       " 14.09,\n",
       " 12.76,\n",
       " 14.86,\n",
       " 16.2,\n",
       " 17.62,\n",
       " 14.41,\n",
       " 14.87,\n",
       " 19.5,\n",
       " 15.76,\n",
       " 16.61,\n",
       " 12.43,\n",
       " 12.02,\n",
       " 12.02,\n",
       " 16.57,\n",
       " 13.28,\n",
       " 12.23,\n",
       " 14.72,\n",
       " 17.11,\n",
       " 18.67,\n",
       " 16.49,\n",
       " 18.44,\n",
       " 11.68,\n",
       " 15.37,\n",
       " 14.52,\n",
       " 13.4,\n",
       " 13.06,\n",
       " 18.37,\n",
       " 16.89,\n",
       " 14.78,\n",
       " 13.06,\n",
       " 12.11,\n",
       " 15.29,\n",
       " 14.3,\n",
       " 13.5,\n",
       " 14.52,\n",
       " 16.8,\n",
       " 15.2,\n",
       " 12.81,\n",
       " 11.88,\n",
       " 13.6,\n",
       " 13.17,\n",
       " 12.17,\n",
       " 14.45,\n",
       " 13.0,\n",
       " 13.89,\n",
       " 11.17,\n",
       " 14.57,\n",
       " 17.99,\n",
       " 15.9,\n",
       " 19.91,\n",
       " 12.29,\n",
       " 10.25,\n",
       " 11.18,\n",
       " 18.12,\n",
       " 11.52,\n",
       " 14.65,\n",
       " 14.92,\n",
       " 11.61,\n",
       " 17.16,\n",
       " 16.97,\n",
       " 13.58,\n",
       " 11.39,\n",
       " 17.59,\n",
       " 17.19,\n",
       " 10.75,\n",
       " 18.27,\n",
       " 18.66,\n",
       " 19.27,\n",
       " 19.27,\n",
       " 13.11,\n",
       " 19.93,\n",
       " 10.33,\n",
       " 19.05,\n",
       " 19.93,\n",
       " 17.29,\n",
       " 14.96,\n",
       " 18.92,\n",
       " 13.53,\n",
       " 14.45,\n",
       " 16.91,\n",
       " 17.36,\n",
       " 12.07,\n",
       " 15.37,\n",
       " 11.2,\n",
       " 18.12,\n",
       " 17.65,\n",
       " 11.73,\n",
       " 16.99,\n",
       " 13.77,\n",
       " 18.53,\n",
       " 10.14,\n",
       " 12.47,\n",
       " 19.37,\n",
       " 14.28,\n",
       " 14.48,\n",
       " 16.1,\n",
       " 16.15,\n",
       " 15.78,\n",
       " 11.56,\n",
       " 10.89,\n",
       " 17.08,\n",
       " 13.14,\n",
       " 16.13,\n",
       " 14.89,\n",
       " 12.02,\n",
       " 11.11,\n",
       " 10.55,\n",
       " 12.03,\n",
       " 16.89,\n",
       " 19.61,\n",
       " 11.35,\n",
       " 17.75,\n",
       " 10.06,\n",
       " 19.35,\n",
       " 14.45,\n",
       " 17.03,\n",
       " 17.36,\n",
       " 14.62,\n",
       " 12.96,\n",
       " 11.3,\n",
       " 15.25,\n",
       " 13.34,\n",
       " 13.14,\n",
       " 15.38,\n",
       " 18.18,\n",
       " 13.33,\n",
       " 11.5,\n",
       " 16.15,\n",
       " 12.28,\n",
       " 17.98,\n",
       " 17.39,\n",
       " 12.23,\n",
       " 18.0,\n",
       " 10.22,\n",
       " 19.46,\n",
       " 12.14,\n",
       " 14.87,\n",
       " 13.25,\n",
       " 18.43,\n",
       " 12.46,\n",
       " 15.43,\n",
       " 17.37,\n",
       " 11.56,\n",
       " 12.22,\n",
       " 17.18,\n",
       " 16.37,\n",
       " 16.62,\n",
       " 19.47,\n",
       " 17.13,\n",
       " 12.67,\n",
       " 19.65,\n",
       " 12.15,\n",
       " 15.73,\n",
       " 17.66,\n",
       " 15.42,\n",
       " 15.84,\n",
       " 16.66,\n",
       " 17.87,\n",
       " 16.55,\n",
       " 16.8,\n",
       " 18.66,\n",
       " 14.08,\n",
       " 17.87,\n",
       " 14.43,\n",
       " 11.41,\n",
       " 17.46,\n",
       " 19.49,\n",
       " 19.46,\n",
       " 12.74,\n",
       " 17.12,\n",
       " 16.81,\n",
       " 14.74,\n",
       " 18.08,\n",
       " 11.39,\n",
       " 18.2,\n",
       " 16.44,\n",
       " 15.38,\n",
       " 18.84,\n",
       " 15.55,\n",
       " 16.18,\n",
       " 18.14,\n",
       " 10.27,\n",
       " 10.59,\n",
       " 18.6,\n",
       " 13.51,\n",
       " 13.36,\n",
       " 17.16,\n",
       " 18.93,\n",
       " 18.36725,\n",
       " 18.67,\n",
       " 18.22,\n",
       " 10.94,\n",
       " 16.08,\n",
       " 16.23,\n",
       " 18.78,\n",
       " 14.84,\n",
       " 11.47,\n",
       " 18.95,\n",
       " 14.3,\n",
       " 11.84,\n",
       " 12.21,\n",
       " 18.79,\n",
       " 15.65,\n",
       " 15.87,\n",
       " 15.75,\n",
       " 18.33,\n",
       " 18.88,\n",
       " 18.45,\n",
       " 11.11,\n",
       " 15.47,\n",
       " 13.87,\n",
       " 18.73,\n",
       " 19.64,\n",
       " 13.48,\n",
       " 10.63,\n",
       " 15.96,\n",
       " 15.69,\n",
       " 13.33,\n",
       " 16.36,\n",
       " 10.88,\n",
       " 16.74,\n",
       " 11.62,\n",
       " 14.01,\n",
       " 14.63,\n",
       " 11.5,\n",
       " 18.22,\n",
       " 19.99,\n",
       " 15.74,\n",
       " 12.17,\n",
       " 19.43,\n",
       " 16.62,\n",
       " 13.47,\n",
       " 12.12,\n",
       " 19.57,\n",
       " 18.82,\n",
       " 18.55,\n",
       " 16.52,\n",
       " 12.21,\n",
       " 14.42,\n",
       " 17.59,\n",
       " 14.21,\n",
       " 10.57,\n",
       " 13.4,\n",
       " 13.61,\n",
       " 16.31,\n",
       " 16.31,\n",
       " 10.69,\n",
       " 13.16,\n",
       " 15.29,\n",
       " 18.93,\n",
       " 11.06,\n",
       " 14.92,\n",
       " 16.34,\n",
       " 15.15,\n",
       " 19.11,\n",
       " 18.38,\n",
       " 16.98,\n",
       " 11.98,\n",
       " 12.9,\n",
       " 15.4,\n",
       " 18.63,\n",
       " 11.44,\n",
       " 17.87,\n",
       " 14.12,\n",
       " 13.07,\n",
       " 17.6,\n",
       " 15.06,\n",
       " 15.81,\n",
       " 17.51,\n",
       " 14.76,\n",
       " 10.87,\n",
       " 17.38,\n",
       " 12.41,\n",
       " 14.86,\n",
       " 19.95,\n",
       " 12.98,\n",
       " 15.28,\n",
       " 16.74,\n",
       " 10.22,\n",
       " 16.59,\n",
       " 13.34,\n",
       " 13.76,\n",
       " 17.75,\n",
       " 18.06,\n",
       " 13.33,\n",
       " 16.71,\n",
       " 10.2,\n",
       " 16.89,\n",
       " 16.94,\n",
       " 13.62,\n",
       " 17.68,\n",
       " 13.12,\n",
       " 10.92,\n",
       " 16.5,\n",
       " 18.04,\n",
       " 15.73,\n",
       " 11.02,\n",
       " 11.85,\n",
       " 19.39,\n",
       " 18.26,\n",
       " 17.82,\n",
       " 16.41,\n",
       " 18.15,\n",
       " 16.03,\n",
       " 14.21,\n",
       " 18.8064375,\n",
       " 14.61,\n",
       " 18.81,\n",
       " 11.95,\n",
       " 17.85,\n",
       " 14.31,\n",
       " 15.39,\n",
       " 17.95,\n",
       " 14.53,\n",
       " 17.45,\n",
       " 14.73,\n",
       " 17.22,\n",
       " 13.89,\n",
       " 18.72,\n",
       " 13.7,\n",
       " 18.3,\n",
       " 15.69,\n",
       " 12.8,\n",
       " 11.17,\n",
       " 12.86,\n",
       " 11.49,\n",
       " 20.0,\n",
       " 15.35,\n",
       " 12.17,\n",
       " 17.87,\n",
       " 11.84,\n",
       " 11.6,\n",
       " 12.49,\n",
       " 16.99,\n",
       " 16.52,\n",
       " 14.56,\n",
       " 13.45,\n",
       " 11.74,\n",
       " 14.55,\n",
       " 12.45,\n",
       " 14.79,\n",
       " 14.36,\n",
       " 13.11,\n",
       " 15.86,\n",
       " 16.61,\n",
       " 13.83,\n",
       " 12.46,\n",
       " 15.68,\n",
       " 18.09,\n",
       " 10.63,\n",
       " 10.19,\n",
       " 16.96,\n",
       " 10.55,\n",
       " 10.68,\n",
       " 20.0,\n",
       " 12.31,\n",
       " 13.82,\n",
       " 10.38,\n",
       " 10.41,\n",
       " 14.43,\n",
       " 12.48,\n",
       " 20.0,\n",
       " 12.91,\n",
       " 12.02,\n",
       " 14.58,\n",
       " 16.15,\n",
       " 11.53,\n",
       " 16.99,\n",
       " 20.0,\n",
       " 17.61,\n",
       " 18.5,\n",
       " 19.7,\n",
       " 14.71,\n",
       " 18.45,\n",
       " 16.77,\n",
       " 17.21,\n",
       " 13.78,\n",
       " 15.16,\n",
       " 18.29,\n",
       " 15.56,\n",
       " 13.18,\n",
       " 15.72,\n",
       " 10.79,\n",
       " 10.64,\n",
       " 19.19,\n",
       " 15.27,\n",
       " 10.69,\n",
       " 19.98,\n",
       " 18.9,\n",
       " 10.72,\n",
       " 17.64,\n",
       " 12.91,\n",
       " 10.98,\n",
       " 18.4,\n",
       " 14.37,\n",
       " 17.24,\n",
       " 10.94,\n",
       " 12.77,\n",
       " 11.71,\n",
       " 15.13,\n",
       " 17.9,\n",
       " 10.72,\n",
       " 11.9,\n",
       " 18.12,\n",
       " 11.86,\n",
       " 20.0,\n",
       " 10.7,\n",
       " 16.69,\n",
       " 12.68,\n",
       " 12.32,\n",
       " 11.2,\n",
       " 17.73,\n",
       " 14.87,\n",
       " 17.37,\n",
       " 14.63,\n",
       " 15.07,\n",
       " 13.09,\n",
       " 14.19,\n",
       " 15.81,\n",
       " 12.82,\n",
       " 11.24,\n",
       " 10.56,\n",
       " 20.0,\n",
       " 13.83,\n",
       " 12.48,\n",
       " 14.94,\n",
       " 12.18]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x / 16000 for x in dataset['train']['input_length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['train'] = dataset['train'].take(1000).map(prepare_dataset, num_proc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset['train'] = dataset['train'].take(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = raw_dataset.remove_columns(['audio', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.load lora model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  distil-whisper \n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"distil-whisper/distil-medium.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.decoder.layers.0.self_attn.k_proj',\n",
       " 'model.decoder.layers.0.self_attn.v_proj',\n",
       " 'model.decoder.layers.0.self_attn.q_proj',\n",
       " 'model.decoder.layers.0.self_attn.out_proj',\n",
       " 'model.decoder.layers.0.self_attn_layer_norm',\n",
       " 'model.decoder.layers.0.encoder_attn.k_proj',\n",
       " 'model.decoder.layers.0.encoder_attn.v_proj',\n",
       " 'model.decoder.layers.0.encoder_attn.q_proj',\n",
       " 'model.decoder.layers.0.encoder_attn.out_proj',\n",
       " 'model.decoder.layers.0.encoder_attn_layer_norm',\n",
       " 'model.decoder.layers.0.fc1',\n",
       " 'model.decoder.layers.0.fc2',\n",
       " 'model.decoder.layers.0.final_layer_norm',\n",
       " 'model.decoder.layers.1.self_attn.k_proj',\n",
       " 'model.decoder.layers.1.self_attn.v_proj',\n",
       " 'model.decoder.layers.1.self_attn.q_proj',\n",
       " 'model.decoder.layers.1.self_attn.out_proj',\n",
       " 'model.decoder.layers.1.self_attn_layer_norm',\n",
       " 'model.decoder.layers.1.encoder_attn.k_proj',\n",
       " 'model.decoder.layers.1.encoder_attn.v_proj',\n",
       " 'model.decoder.layers.1.encoder_attn.q_proj',\n",
       " 'model.decoder.layers.1.encoder_attn.out_proj',\n",
       " 'model.decoder.layers.1.encoder_attn_layer_norm',\n",
       " 'model.decoder.layers.1.fc1',\n",
       " 'model.decoder.layers.1.fc2',\n",
       " 'model.decoder.layers.1.final_layer_norm',\n",
       " 'model.decoder.layer_norm']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_modules = []\n",
    "keywords = [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"]\n",
    "for id, (name, param) in enumerate(model.named_modules()):\n",
    "    if 'model.decoder' in name and (any(keyword in name for keyword in keywords)):\n",
    "        target_modules.append(name)\n",
    "        \n",
    "target_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LoRA \n",
    "lora_config = LoraConfig(\n",
    "    r=8,                    # Rank \n",
    "    lora_alpha=32,           # alpha\n",
    "    target_modules=target_modules,  # LoRA\n",
    "    lora_dropout=0.1,        # dropout\n",
    "    bias=\"none\",             #  bias\n",
    ")\n",
    "\n",
    "#  LoRA \n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 425,984 || all params: 394,801,152 || trainable%: 0.1079\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_layers = filter(lambda p: p.requires_grad, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<filter at 0x73e542657670>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425984"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425984"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for group in optimizer.param_groups for p in group['params'] if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpds1/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./model\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=1000,\n",
    "    # max_steps=100, # only for testing purposes, remove this from your final run :)\n",
    "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
    "    label_names=[\"labels\"],  # same reason as above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor ([`Wav2Vec2Processor`])\n",
    "            The processor used for proccessing the data.\n",
    "        decoder_start_token_id (:obj: `int`)\n",
    "            The start-of-sequence token id of the decoder.\n",
    "        decoder_prev_token_id (:obj: `int`)\n",
    "            The start-of-prompt token id of the decoder\n",
    "        input_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned input sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        target_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned target sequences (according to the model's padding side and padding index).\n",
    "            See above for details.\n",
    "        max_target_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` of the returned list and optionally padding length (see above).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "    decoder_prev_token_id: int\n",
    "    input_padding: Union[bool, str] = \"max_length\"\n",
    "    target_padding: Union[bool, str] = \"max_length\"\n",
    "    max_target_length: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], np.ndarray]]]) -> Dict[str, np.ndarray]:\n",
    "        # split inputs and labels since they have to be of different lengths and need\n",
    "        # different padding methods\n",
    "\n",
    "        # dataloader returns a list of features which we convert to a dict\n",
    "        input_features = {\"input_features\": [feature[\"input_features\"] for feature in features]}\n",
    "        label_features = {\"input_ids\": [feature[\"labels\"] for feature in features]}\n",
    "\n",
    "        # reformat list to dict and set to pytorch format\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.input_padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            max_length=self.max_target_length,\n",
    "            padding=self.target_padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # shift labels to the right to get decoder input ids\n",
    "        labels = labels_batch[\"input_ids\"]\n",
    "        decoder_input_ids = labels[:, :-1]\n",
    "        labels = labels[:, 1:]\n",
    "        labels_mask = labels_batch.attention_mask[:, 1:]\n",
    "\n",
    "        # replace padding with -100 to ignore correctly when computing the loss\n",
    "        labels = labels.masked_fill(labels_mask.ne(1), -100)\n",
    "\n",
    "        # replace initial prompt tokens with -100 to ignore correctly when computing the loss\n",
    "        bos_index = torch.argmax((labels == self.decoder_start_token_id).long(), dim=1)\n",
    "        bos_index = torch.where(bos_index > 0, bos_index + 1, bos_index)\n",
    "        prompt_mask = torch.arange(labels.shape[1]) < bos_index[:, None]\n",
    "        labels = torch.where(prompt_mask, -100, labels)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"decoder_input_ids\"] = decoder_input_ids\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import preprocess_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"./checkpoint-25000-epoch-1\" # Use the same model ID as before.\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    peft_config.base_model_name_or_path\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): WhisperForConditionalGeneration(\n",
       "      (model): WhisperModel(\n",
       "        (encoder): WhisperEncoder(\n",
       "          (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (embed_positions): Embedding(1500, 1024)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x WhisperEncoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): WhisperDecoder(\n",
       "          (embed_tokens): Embedding(51864, 1024, padding_idx=50256)\n",
       "          (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x WhisperDecoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): WhisperSdpaAttention(\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=1024, out_features=51864, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0edfe907a3048f9910df7b88b48073c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc18535163140e09888fb3885f15ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0588b02428d84ce29851a0e67895acfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = preprocess_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'text', 'input_features', 'input_length', 'labels'],\n",
       "        num_rows: 1061465\n",
       "    })\n",
       "    ID_eval: Dataset({\n",
       "        features: ['audio', 'text', 'input_features', 'input_length', 'labels'],\n",
       "        num_rows: 19567\n",
       "    })\n",
       "    OOD_eval: Dataset({\n",
       "        features: ['audio', 'text', 'input_features', 'input_length', 'labels'],\n",
       "        num_rows: 4077\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"openai/whisper-medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(base_model)\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_start_token_id = model.config.decoder_start_token_id  # <|startoftranscript|>\n",
    "decoder_prev_token_id = tokenizer.all_special_ids[-3]  # <|startofprev|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=decoder_start_token_id,\n",
    "    decoder_prev_token_id=decoder_prev_token_id,\n",
    "    input_padding=\"longest\",\n",
    "    target_padding=\"max_length\",\n",
    "    max_target_length=448,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'text', 'input_features', 'input_length', 'labels'],\n",
       "        num_rows: 1061465\n",
       "    })\n",
       "    ID_eval: Dataset({\n",
       "        features: ['audio', 'text', 'input_features', 'input_length', 'labels'],\n",
       "        num_rows: 19567\n",
       "    })\n",
       "    OOD_eval: Dataset({\n",
       "        features: ['audio', 'text', 'input_features', 'input_length', 'labels'],\n",
       "        num_rows: 4077\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset['train'],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=72,\n",
    "    drop_last=False,\n",
    "    num_workers=8,\n",
    "    # pin_memory=training_args.dataloader_pin_memory,\n",
    ")\n",
    "\n",
    "ID_dataloader = DataLoader(\n",
    "    dataset['ID_eval'],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=72,\n",
    "    drop_last=False,\n",
    "    num_workers=8,\n",
    "    # pin_memory=training_args.dataloader_pin_memory,\n",
    ")\n",
    "\n",
    "OOD_dataloader = DataLoader(\n",
    "    dataset['OOD_eval'],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=72,\n",
    "    drop_last=False,\n",
    "    num_workers=8,\n",
    "    # pin_memory=training_args.dataloader_pin_memory,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"max_length\": 128,\n",
    "    \"num_beams\": 5,\n",
    "    # \"language\": 'de', \n",
    "    # \"task\": 'transcription',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa45f24254f41fdb001617b6c330ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating train_eval...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_predictions = []\n",
    "train_references = []\n",
    "train_normalized_predictions = []\n",
    "train_normalized_references = []\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "for batch in tqdm(\n",
    "    islice(train_dataloader, 100),\n",
    "    desc=f\"Evaluating {'train_eval'}...\",\n",
    "    ):\n",
    "    generated_ids = model.generate(batch[\"input_features\"].to('cuda'), **gen_kwargs)\n",
    "    labels = batch[\"labels\"]\n",
    "    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    train_predictions.extend(decoded_preds)\n",
    "    train_references.extend(decoded_labels)\n",
    "    train_normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
    "    train_normalized_references.extend([normalizer(label).strip() for label in decoded_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb03e0779294afb9c168110c827087c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating ID_eval...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id_predictions = []\n",
    "id_references = []\n",
    "id_normalized_predictions = []\n",
    "id_normalized_references = []\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "ID_dataloader = islice(ID_dataloader, 100)\n",
    "\n",
    "for batch in tqdm(\n",
    "    ID_dataloader,\n",
    "    desc=f\"Evaluating {'ID_eval'}...\",\n",
    "    ):\n",
    "    generated_ids = model.generate(batch[\"input_features\"].to(\"cuda\"), **gen_kwargs)\n",
    "    labels = batch[\"labels\"]\n",
    "    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    id_predictions.extend(decoded_preds)\n",
    "    id_references.extend(decoded_labels)\n",
    "    id_normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
    "    id_normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n",
    "    # del generated_ids, labels, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4eaff25cf049af988b377da8e8f75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating OOD_eval...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ood_predictions = []\n",
    "ood_references = []\n",
    "ood_normalized_predictions = []\n",
    "ood_normalized_references = []\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "OOD_dataloader = islice(OOD_dataloader, 100)\n",
    "\n",
    "for batch in tqdm(\n",
    "    OOD_dataloader,\n",
    "    desc=f\"Evaluating {'OOD_eval'}...\",\n",
    "    ):\n",
    "    generated_ids = model.generate(batch[\"input_features\"].to('cuda'), **gen_kwargs)\n",
    "    labels = batch[\"labels\"]\n",
    "    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    ood_predictions.extend(decoded_preds)\n",
    "    ood_references.extend(decoded_labels)\n",
    "    ood_normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
    "    ood_normalized_references.extend([normalizer(label).strip() for label in decoded_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: WER: 29.05259283117839, Normalized WER: 27.846659532181622\n"
     ]
    }
   ],
   "source": [
    "wer = 100 * metric.compute(predictions=train_predictions, references=train_references)\n",
    "normalized_wer = 100 * metric.compute(predictions=train_normalized_predictions, references=train_normalized_references)\n",
    "\n",
    "print(f\"train: WER: {wer}, Normalized WER: {normalized_wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 38.64809810011698, Normalized WER: 36.10389349812899\n"
     ]
    }
   ],
   "source": [
    "wer = 100 * metric.compute(predictions=id_predictions, references=id_references)\n",
    "normalized_wer = 100 * metric.compute(predictions=id_normalized_predictions, references=id_normalized_references)\n",
    "\n",
    "print(f\"ID: WER: {wer}, Normalized WER: {normalized_wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4077"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_predictions_new, ood_references_new = zip(*[(x, y) for x, y in zip(ood_predictions, ood_references) if x != \"\" and y != \"\"])\n",
    "ood_normalized_predictions_new, ood_normalized_references_new = zip(*[(x, y) for x, y in zip(ood_normalized_predictions, ood_normalized_references) if x != \"\" and y != \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOD: WER: 63.56350110984462, Normalized WER: 50.20510483135825\n"
     ]
    }
   ],
   "source": [
    "wer = 100 * metric.compute(predictions=ood_predictions_new, references=ood_references_new)\n",
    "normalized_wer = 100 * metric.compute(predictions=ood_normalized_predictions_new, references=ood_normalized_references_new)\n",
    "\n",
    "print(f\"OOD: WER: {wer}, Normalized WER: {normalized_wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"distil-whisper/distil-medium.en\").to(\"cuda\")\n",
    "\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(\"distil-whisper/distil-medium.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset['train'],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=72,\n",
    "    drop_last=False,\n",
    "    num_workers=8,\n",
    "    # pin_memory=training_args.dataloader_pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d9795f52e7402e9e86485a23d2645b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating train_eval...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 19\u001b[0m\n\u001b[1;32m      8\u001b[0m gen_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# \"language\": 'de', \u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# \"task\": 'transcription',\u001b[39;00m\n\u001b[1;32m     13\u001b[0m }\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     16\u001b[0m     islice(train_dataloader, \u001b[38;5;241m100\u001b[39m),\n\u001b[1;32m     17\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_eval\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     ):\n\u001b[0;32m---> 19\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     21\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, labels, tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:671\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate\u001b[0;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, return_token_timestamps, return_segments, return_dict_in_generate, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m             proc\u001b[38;5;241m.\u001b[39mset_begin_index(decoder_input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    664\u001b[0m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[1;32m    665\u001b[0m (\n\u001b[1;32m    666\u001b[0m     seek_sequences,\n\u001b[1;32m    667\u001b[0m     seek_outputs,\n\u001b[1;32m    668\u001b[0m     should_skip,\n\u001b[1;32m    669\u001b[0m     do_condition_on_prev_tokens,\n\u001b[1;32m    670\u001b[0m     model_output_type,\n\u001b[0;32m--> 671\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:832\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate_with_fallback\u001b[0;34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    829\u001b[0m             generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, batch_size \u001b[38;5;241m-\u001b[39m cur_bsz), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    830\u001b[0m         )\n\u001b[0;32m--> 832\u001b[0m seek_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m model_output_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/transformers/generation/utils.py:2063\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2055\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2056\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2057\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2058\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2059\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2060\u001b[0m     )\n\u001b[1;32m   2062\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2063\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2076\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2077\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2078\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2084\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2085\u001b[0m     )\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/transformers/generation/utils.py:3251\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   3246\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m   3247\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[1;32m   3248\u001b[0m     next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3249\u001b[0m )  \u001b[38;5;66;03m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[0;32m-> 3251\u001b[0m next_token_scores_processed \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_sample:\n\u001b[1;32m   3253\u001b[0m     next_token_scores_processed \u001b[38;5;241m=\u001b[39m logits_warper(input_ids, next_token_scores_processed)\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/transformers/generation/logits_process.py:98\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/workspace/services/distil-whisper/venv/lib/python3.12/site-packages/transformers/generation/logits_process.py:1836\u001b[0m, in \u001b[0;36mSuppressTokensLogitsProcessor.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;129m@add_start_docstrings\u001b[39m(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m   1835\u001b[0m     vocab_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mscores\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1836\u001b[0m     suppress_token_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppress_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1837\u001b[0m     scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(suppress_token_mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m), scores)\n\u001b[1;32m   1838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_predictions = []\n",
    "train_references = []\n",
    "train_normalized_predictions = []\n",
    "train_normalized_references = []\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "gen_kwargs = {\n",
    "    \"max_length\": 128,\n",
    "    \"num_beams\": 5,\n",
    "    # \"language\": 'de', \n",
    "    # \"task\": 'transcription',\n",
    "}\n",
    "\n",
    "for batch in tqdm(\n",
    "    islice(train_dataloader, 100),\n",
    "    desc=f\"Evaluating {'train_eval'}...\",\n",
    "    ):\n",
    "    generated_ids = model.generate(batch[\"input_features\"].to('cuda'), **gen_kwargs)\n",
    "    labels = batch[\"labels\"]\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    train_predictions.extend(decoded_preds)\n",
    "    train_references.extend(decoded_labels)\n",
    "    train_normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
    "    train_normalized_references.extend([normalizer(label).strip() for label in decoded_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: WER: 113.99646630496858, Normalized WER: 118.0535665044787\n"
     ]
    }
   ],
   "source": [
    "wer = 100 * metric.compute(predictions=train_predictions, references=train_references)\n",
    "normalized_wer = 100 * metric.compute(predictions=train_normalized_predictions, references=train_normalized_references)\n",
    "\n",
    "print(f\"train: WER: {wer}, Normalized WER: {normalized_wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\").to(\"cpu\")\n",
    "model2 = WhisperForConditionalGeneration.from_pretrained(\"distil-whisper/distil-medium.en\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(51865, 1024, padding_idx=50257)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.model.decoder.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperConfig {\n",
       "  \"_name_or_path\": \"distil-whisper/distil-medium.en\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"apply_spec_augment\": false,\n",
       "  \"architectures\": [\n",
       "    \"WhisperForConditionalGeneration\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"begin_suppress_tokens\": [\n",
       "    220,\n",
       "    50256\n",
       "  ],\n",
       "  \"bos_token_id\": 50257,\n",
       "  \"classifier_proj_size\": 256,\n",
       "  \"d_model\": 1024,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 4096,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 2,\n",
       "  \"decoder_start_token_id\": 50257,\n",
       "  \"dropout\": 0.0,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 4096,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 24,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"forced_decoder_ids\": [\n",
       "    [\n",
       "      1,\n",
       "      50362\n",
       "    ]\n",
       "  ],\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"mask_feature_length\": 10,\n",
       "  \"mask_feature_min_masks\": 0,\n",
       "  \"mask_feature_prob\": 0.0,\n",
       "  \"mask_time_length\": 10,\n",
       "  \"mask_time_min_masks\": 2,\n",
       "  \"mask_time_prob\": 0.05,\n",
       "  \"max_length\": 448,\n",
       "  \"max_source_positions\": 1500,\n",
       "  \"max_target_positions\": 448,\n",
       "  \"median_filter_width\": 7,\n",
       "  \"model_type\": \"whisper\",\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"num_mel_bins\": 80,\n",
       "  \"pad_token_id\": 50256,\n",
       "  \"scale_embedding\": false,\n",
       "  \"suppress_tokens\": [\n",
       "    1,\n",
       "    2,\n",
       "    7,\n",
       "    8,\n",
       "    9,\n",
       "    10,\n",
       "    14,\n",
       "    25,\n",
       "    26,\n",
       "    27,\n",
       "    28,\n",
       "    29,\n",
       "    31,\n",
       "    58,\n",
       "    59,\n",
       "    60,\n",
       "    61,\n",
       "    62,\n",
       "    63,\n",
       "    90,\n",
       "    91,\n",
       "    92,\n",
       "    93,\n",
       "    357,\n",
       "    366,\n",
       "    438,\n",
       "    532,\n",
       "    685,\n",
       "    705,\n",
       "    796,\n",
       "    930,\n",
       "    1058,\n",
       "    1220,\n",
       "    1267,\n",
       "    1279,\n",
       "    1303,\n",
       "    1343,\n",
       "    1377,\n",
       "    1391,\n",
       "    1635,\n",
       "    1782,\n",
       "    1875,\n",
       "    2162,\n",
       "    2361,\n",
       "    2488,\n",
       "    3467,\n",
       "    4008,\n",
       "    4211,\n",
       "    4600,\n",
       "    4808,\n",
       "    5299,\n",
       "    5855,\n",
       "    6329,\n",
       "    7203,\n",
       "    9609,\n",
       "    9959,\n",
       "    10563,\n",
       "    10786,\n",
       "    11420,\n",
       "    11709,\n",
       "    11907,\n",
       "    13163,\n",
       "    13697,\n",
       "    13700,\n",
       "    14808,\n",
       "    15306,\n",
       "    16410,\n",
       "    16791,\n",
       "    17992,\n",
       "    19203,\n",
       "    19510,\n",
       "    20724,\n",
       "    22305,\n",
       "    22935,\n",
       "    27007,\n",
       "    30109,\n",
       "    30420,\n",
       "    33409,\n",
       "    34949,\n",
       "    40283,\n",
       "    40493,\n",
       "    40549,\n",
       "    47282,\n",
       "    49146,\n",
       "    50257,\n",
       "    50357,\n",
       "    50358,\n",
       "    50359,\n",
       "    50360,\n",
       "    50361\n",
       "  ],\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.44.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_weighted_layer_sum\": false,\n",
       "  \"vocab_size\": 51864\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51864, 1024, padding_idx=50256)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1024, out_features=51864, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = WhisperTokenizerFast.from_pretrained(\"openai/whisper-medium\")\n",
    "tokenizer2 = WhisperTokenizerFast.from_pretrained(\"openai/whisper-medium.en\")\n",
    "tokenizer3 = WhisperTokenizerFast.from_pretrained(\"distil-whisper/distil-medium.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer4 = WhisperTokenizerFast.from_pretrained(\"openai/whisper-large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " '',\n",
       " 'gleich',\n",
       " 'ludzi',\n",
       " '',\n",
       " 'ez',\n",
       " 'responder',\n",
       " 'conomique',\n",
       " 'nasl',\n",
       " '',\n",
       " 'butts',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Oke',\n",
       " 'owym',\n",
       " '',\n",
       " 'aphrag',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'leva',\n",
       " '',\n",
       " '',\n",
       " 'b',\n",
       " 'tes',\n",
       " 'bastards',\n",
       " '',\n",
       " '',\n",
       " 'Jeg',\n",
       " 'passt',\n",
       " '',\n",
       " '',\n",
       " 'ging',\n",
       " 'segue',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'irrespons',\n",
       " '',\n",
       " 'Its',\n",
       " 'onya',\n",
       " 'bana',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'rire',\n",
       " 'Avo',\n",
       " '',\n",
       " '?...',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Dlatego',\n",
       " 'hu',\n",
       " 'atif',\n",
       " '',\n",
       " 'as',\n",
       " 'probabil',\n",
       " '',\n",
       " 'quoi',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'nd',\n",
       " 'dowiad',\n",
       " 'imaginar',\n",
       " 'blev',\n",
       " 'r',\n",
       " 'on',\n",
       " 'gogg',\n",
       " 'erkt',\n",
       " 'contrat',\n",
       " 'Jeez',\n",
       " 'Schn',\n",
       " 'widz',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'astronom',\n",
       " 'c',\n",
       " '',\n",
       " 'uen',\n",
       " '',\n",
       " '',\n",
       " 'ih',\n",
       " 'Ziel',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'sst',\n",
       " '',\n",
       " '',\n",
       " 'falou',\n",
       " '',\n",
       " 'k',\n",
       " 'erting',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'devam',\n",
       " 'ORIA',\n",
       " '',\n",
       " 'principio',\n",
       " 'ava',\n",
       " 'anyhow',\n",
       " '',\n",
       " '',\n",
       " 'Tensor',\n",
       " 'sai',\n",
       " 'expres',\n",
       " 'VISTA',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'desenvol',\n",
       " 'argu',\n",
       " 'utar',\n",
       " '',\n",
       " '',\n",
       " 'Maintenant',\n",
       " '',\n",
       " 'frher',\n",
       " '',\n",
       " 'Daha',\n",
       " 'geschafft',\n",
       " '',\n",
       " 'nacional',\n",
       " 'Sicht',\n",
       " 'sist',\n",
       " 'gaat',\n",
       " '',\n",
       " 'hacerlo',\n",
       " '',\n",
       " 'ningen',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'prze',\n",
       " 'WAN',\n",
       " '',\n",
       " 'y',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'irement',\n",
       " 'lemonade',\n",
       " 'plo',\n",
       " 'adora',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'cosplay',\n",
       " 'elier',\n",
       " 'Wohn',\n",
       " '',\n",
       " '',\n",
       " 'rzeczywicie',\n",
       " '',\n",
       " 'Sovi',\n",
       " '',\n",
       " 'oscillator',\n",
       " '',\n",
       " 'yar',\n",
       " 'vidos',\n",
       " 'tril',\n",
       " 'JESS',\n",
       " 'ligt',\n",
       " 'venidos',\n",
       " '',\n",
       " 'llam',\n",
       " 'hr',\n",
       " '',\n",
       " '',\n",
       " 'Kristin',\n",
       " '],,',\n",
       " 'dif',\n",
       " 'kap',\n",
       " '',\n",
       " 'todas',\n",
       " 'beste',\n",
       " 'ch',\n",
       " 'pniej',\n",
       " 'amar',\n",
       " 'Quem',\n",
       " 'begr',\n",
       " '',\n",
       " 'besonders',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'encima',\n",
       " '',\n",
       " 'necesita',\n",
       " 'kull',\n",
       " '',\n",
       " 'purl',\n",
       " '',\n",
       " 'seus',\n",
       " 'wsp',\n",
       " '',\n",
       " 'z',\n",
       " '',\n",
       " '',\n",
       " 'Gobierno',\n",
       " 'ughters',\n",
       " '',\n",
       " '!..',\n",
       " '',\n",
       " 'rdu',\n",
       " 'statt',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'ces',\n",
       " 'assistir',\n",
       " 'xim',\n",
       " '',\n",
       " '',\n",
       " 'ichtlich',\n",
       " 'partir',\n",
       " '',\n",
       " 'koy',\n",
       " '',\n",
       " 'avilion',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'ns',\n",
       " 'cribing',\n",
       " 'orchestral',\n",
       " 'ami',\n",
       " 'besten',\n",
       " '',\n",
       " '',\n",
       " 'diye',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'iss',\n",
       " 'y',\n",
       " 'aca',\n",
       " '',\n",
       " 'Pronunciation',\n",
       " 'erca',\n",
       " 'marc',\n",
       " 'anh',\n",
       " 'auf',\n",
       " 'remo',\n",
       " 'Joanna',\n",
       " 'ivat',\n",
       " '',\n",
       " 'k',\n",
       " 'kaa',\n",
       " 'Hye',\n",
       " 'eyler',\n",
       " 'Whoo',\n",
       " 'pon',\n",
       " 'owy',\n",
       " '',\n",
       " '',\n",
       " 'onc',\n",
       " 'AirPods',\n",
       " 'hacer',\n",
       " '',\n",
       " 'peine',\n",
       " '',\n",
       " '',\n",
       " 'Kont',\n",
       " 'prototy',\n",
       " 'tud',\n",
       " 'istling',\n",
       " 'ihin',\n",
       " 'devenir',\n",
       " 'bla',\n",
       " '',\n",
       " 'icie',\n",
       " '',\n",
       " '',\n",
       " 'bam',\n",
       " 'crates',\n",
       " '',\n",
       " 'Thats',\n",
       " 'hypoth',\n",
       " 'permitir',\n",
       " 'gla',\n",
       " 'handout',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'inta',\n",
       " '',\n",
       " 'reconoc',\n",
       " '',\n",
       " 'vita',\n",
       " 'i',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Scandin',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'cessors',\n",
       " 'thermometer',\n",
       " 'hvis',\n",
       " 'parano',\n",
       " '',\n",
       " 'ihm',\n",
       " '',\n",
       " '',\n",
       " 'ej',\n",
       " '',\n",
       " 'rigt',\n",
       " 'communic',\n",
       " 't',\n",
       " 'lk',\n",
       " '',\n",
       " '',\n",
       " 'empresas',\n",
       " 'dcouvrir',\n",
       " 'apro',\n",
       " 'MAL',\n",
       " '',\n",
       " '',\n",
       " 'mniej',\n",
       " 'seni',\n",
       " '',\n",
       " '',\n",
       " 'conhecer',\n",
       " 'leider',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Recht',\n",
       " '',\n",
       " '~\"',\n",
       " 'asmine',\n",
       " 'kannt',\n",
       " 'ungs',\n",
       " 'olmak',\n",
       " 'alen',\n",
       " '(?)',\n",
       " '',\n",
       " '',\n",
       " 'dn',\n",
       " 'plante',\n",
       " 'respe',\n",
       " '',\n",
       " 'baar',\n",
       " 'dua',\n",
       " 'famille',\n",
       " 'v',\n",
       " 'md',\n",
       " 'qu',\n",
       " '',\n",
       " 'Forgive',\n",
       " 'prcis',\n",
       " 'aj',\n",
       " 'lequel',\n",
       " '',\n",
       " '',\n",
       " 'Audience',\n",
       " 'detta',\n",
       " 'waffle',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'iamente',\n",
       " 'tanta',\n",
       " '',\n",
       " 'natomiast',\n",
       " 'lais',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'preguntas',\n",
       " '',\n",
       " 'entonces',\n",
       " 'jeeli',\n",
       " 'ange',\n",
       " '',\n",
       " '',\n",
       " 'aczy',\n",
       " 'estab',\n",
       " 'personaje',\n",
       " '',\n",
       " 'esus',\n",
       " '',\n",
       " 'Jimin',\n",
       " 'wides',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'erzh',\n",
       " 'astes',\n",
       " '',\n",
       " '',\n",
       " 'mj',\n",
       " 'jzy',\n",
       " 'alg',\n",
       " '',\n",
       " 'desem',\n",
       " 'EER',\n",
       " 'uestos',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'forcment',\n",
       " 's',\n",
       " 'manera',\n",
       " '',\n",
       " 'brac',\n",
       " '',\n",
       " 'glaze',\n",
       " 'algum',\n",
       " 'haters',\n",
       " '',\n",
       " 'avanz',\n",
       " '',\n",
       " 'KENNETH',\n",
       " 'tutto',\n",
       " 'stuffs',\n",
       " 'allerdings',\n",
       " 'vu',\n",
       " 'r',\n",
       " 'ustering',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Biz',\n",
       " 'metall',\n",
       " '',\n",
       " '',\n",
       " 'gente',\n",
       " 'ieli',\n",
       " 'po',\n",
       " 'podr',\n",
       " '',\n",
       " 'voud',\n",
       " 'eft',\n",
       " 'tiene',\n",
       " '',\n",
       " '',\n",
       " 'aquest',\n",
       " 'realidad',\n",
       " 'jum',\n",
       " '',\n",
       " '',\n",
       " 'ramatic',\n",
       " 'quar',\n",
       " '',\n",
       " 'wok',\n",
       " '',\n",
       " 'eins',\n",
       " 'siempre',\n",
       " 'zijn',\n",
       " 'Sono',\n",
       " 'verndert',\n",
       " 'irr',\n",
       " 'calend',\n",
       " 'jeszcze',\n",
       " 'ander',\n",
       " '',\n",
       " '',\n",
       " 'olmas',\n",
       " 'rico',\n",
       " '',\n",
       " '',\n",
       " 'possvel',\n",
       " 'cul',\n",
       " '',\n",
       " '',\n",
       " 'YJ',\n",
       " '',\n",
       " 'inao',\n",
       " '',\n",
       " '',\n",
       " 'seria',\n",
       " 'ruk',\n",
       " 'cht',\n",
       " '',\n",
       " 'c',\n",
       " '',\n",
       " '',\n",
       " 'hade',\n",
       " 'hiss',\n",
       " '',\n",
       " 'Aunque',\n",
       " '',\n",
       " '',\n",
       " 'uk',\n",
       " 'zurck',\n",
       " 'viscosity',\n",
       " 'avait',\n",
       " 'ncess',\n",
       " 'negcio',\n",
       " '',\n",
       " 't',\n",
       " '',\n",
       " '',\n",
       " 'complet',\n",
       " 'birthdays',\n",
       " 'avors',\n",
       " 'raj',\n",
       " 'baix',\n",
       " 'gemeinsam',\n",
       " '',\n",
       " 'MUR',\n",
       " 'lekker',\n",
       " 'g',\n",
       " 'JUDY',\n",
       " 'logar',\n",
       " 'fhrt',\n",
       " 'maana',\n",
       " 'nied',\n",
       " 'cuz',\n",
       " 'prix',\n",
       " '',\n",
       " 'abajo',\n",
       " '',\n",
       " 'moonlight',\n",
       " '',\n",
       " '',\n",
       " 'humano',\n",
       " '',\n",
       " 'niejsze',\n",
       " '',\n",
       " '',\n",
       " 'citoy',\n",
       " '',\n",
       " 'luz',\n",
       " '',\n",
       " 'pizzas',\n",
       " 'zub',\n",
       " 'pouquinho',\n",
       " '',\n",
       " 'venge',\n",
       " 'Sny',\n",
       " 'viendo',\n",
       " 'consomm',\n",
       " '',\n",
       " '',\n",
       " 'camb',\n",
       " '',\n",
       " '',\n",
       " 'mondo',\n",
       " '',\n",
       " 'Parlament',\n",
       " 'diyorsun',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'simplemente',\n",
       " '',\n",
       " 'anne',\n",
       " 'sera',\n",
       " '',\n",
       " '',\n",
       " 'sach',\n",
       " 'wszystk',\n",
       " '',\n",
       " 'zeni',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'peut',\n",
       " '',\n",
       " '',\n",
       " 'aquesta',\n",
       " 'pyram',\n",
       " '',\n",
       " '',\n",
       " 'gangen',\n",
       " 'swo',\n",
       " '',\n",
       " 'jakie',\n",
       " '',\n",
       " 'Nove',\n",
       " 'pronto',\n",
       " 'gend',\n",
       " 'bers',\n",
       " 'Bueno',\n",
       " '',\n",
       " '',\n",
       " 'impos',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'pi',\n",
       " 'sociedad',\n",
       " '',\n",
       " 'abre',\n",
       " 'jag',\n",
       " 'lorsque',\n",
       " 'olut',\n",
       " '',\n",
       " 'terminar',\n",
       " 'BRUNO',\n",
       " '',\n",
       " 'ud',\n",
       " 'aproxim',\n",
       " 'mote',\n",
       " 'immort',\n",
       " 'emente',\n",
       " 'G',\n",
       " '',\n",
       " 'ordo',\n",
       " 'usement',\n",
       " 'eler',\n",
       " 'anci',\n",
       " '',\n",
       " '',\n",
       " 'andar',\n",
       " 'recipro',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'ng',\n",
       " 'rn',\n",
       " '',\n",
       " 'izar',\n",
       " '',\n",
       " 'Hazrat',\n",
       " 'jae',\n",
       " 'legg',\n",
       " '',\n",
       " 'si',\n",
       " 'kab',\n",
       " '',\n",
       " 'Beni',\n",
       " '',\n",
       " 'Audience',\n",
       " '',\n",
       " 'odu',\n",
       " '',\n",
       " 'grille',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'zysta',\n",
       " '',\n",
       " 'brasile',\n",
       " 'JIM',\n",
       " '',\n",
       " 'istedi',\n",
       " '',\n",
       " 'k',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'SPEAK',\n",
       " '',\n",
       " 'hehe',\n",
       " '',\n",
       " '',\n",
       " 'Monst',\n",
       " '',\n",
       " '',\n",
       " 'toolkit',\n",
       " 'usic',\n",
       " '',\n",
       " '',\n",
       " 'tte',\n",
       " '',\n",
       " 'interessante',\n",
       " 'utet',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Manh',\n",
       " 'iemand',\n",
       " 'como',\n",
       " '',\n",
       " 'gic',\n",
       " '',\n",
       " 'recher',\n",
       " 'banc',\n",
       " 'VAN',\n",
       " 'kum',\n",
       " 'tawa',\n",
       " 'demand',\n",
       " 'wszystko',\n",
       " '',\n",
       " 'esser',\n",
       " '',\n",
       " 'verbess',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Ist',\n",
       " 'imas',\n",
       " 'ew',\n",
       " '',\n",
       " 'redo',\n",
       " 'bardziej',\n",
       " 'beige',\n",
       " 'unknowns',\n",
       " 'pergunta',\n",
       " 'felic',\n",
       " 'roasting',\n",
       " '',\n",
       " 'rans',\n",
       " '',\n",
       " 'lle',\n",
       " '',\n",
       " 'P',\n",
       " '',\n",
       " 'figura',\n",
       " 'Weil',\n",
       " '',\n",
       " 'DAVID',\n",
       " 'iggling',\n",
       " 'n',\n",
       " 'roommates',\n",
       " 'i',\n",
       " 'kytt',\n",
       " 'episod',\n",
       " '',\n",
       " 'nehmen',\n",
       " 'welling',\n",
       " 'Elo',\n",
       " 'questa',\n",
       " 'algo',\n",
       " '',\n",
       " 'czowie',\n",
       " 'elimin',\n",
       " '',\n",
       " '',\n",
       " 'novamente',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'erf',\n",
       " 'strony',\n",
       " '',\n",
       " 'crowd',\n",
       " 'FER',\n",
       " '',\n",
       " '',\n",
       " 'chs',\n",
       " '',\n",
       " '.',\n",
       " 'zobaczy',\n",
       " 'RISADAS',\n",
       " 'ao',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'v',\n",
       " '',\n",
       " '',\n",
       " 'sweetheart',\n",
       " '',\n",
       " 'vara',\n",
       " '',\n",
       " '',\n",
       " 'einfach',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'MEL',\n",
       " '',\n",
       " 'ogr',\n",
       " '',\n",
       " 'entre',\n",
       " 'trainee',\n",
       " 'Sheng',\n",
       " '',\n",
       " 'fry',\n",
       " '',\n",
       " 'deuts',\n",
       " '',\n",
       " 'entreprises',\n",
       " 'punkt',\n",
       " 'mun',\n",
       " '',\n",
       " '',\n",
       " 'ms',\n",
       " 'Cin',\n",
       " 'momentos',\n",
       " '',\n",
       " '',\n",
       " 'Erde',\n",
       " 'Arsen',\n",
       " '',\n",
       " 'enseful',\n",
       " 't',\n",
       " '',\n",
       " '',\n",
       " 'ordum',\n",
       " 'pong',\n",
       " '',\n",
       " 'histoire',\n",
       " 'Goodness',\n",
       " '',\n",
       " 'microf',\n",
       " 'izacin',\n",
       " 'voll',\n",
       " '',\n",
       " '',\n",
       " 'tuvo',\n",
       " 'kt',\n",
       " 'aad',\n",
       " 'wa',\n",
       " 'coloss',\n",
       " '',\n",
       " 'J',\n",
       " 'znaj',\n",
       " 'forder',\n",
       " 'DIRE',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'ieten',\n",
       " 'erecht',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'pastel',\n",
       " 'phi',\n",
       " '',\n",
       " 'modelo',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'commen',\n",
       " '',\n",
       " 'Cuando',\n",
       " '',\n",
       " 'tehd',\n",
       " 'ck',\n",
       " 'kein',\n",
       " 'vem',\n",
       " 'Antes',\n",
       " 'ombres',\n",
       " 'ese',\n",
       " '',\n",
       " 'buat',\n",
       " '',\n",
       " '',\n",
       " 'kommer',\n",
       " '',\n",
       " 'passo',\n",
       " 'schlim',\n",
       " 'Chanel',\n",
       " 'ulen',\n",
       " 'nyt',\n",
       " '',\n",
       " '',\n",
       " 'mue',\n",
       " '',\n",
       " '',\n",
       " 's',\n",
       " 'RIA',\n",
       " '',\n",
       " '',\n",
       " 'motherf',\n",
       " 'ugo',\n",
       " '',\n",
       " 'fon',\n",
       " '',\n",
       " 'devient',\n",
       " '',\n",
       " '',\n",
       " 'plataforma',\n",
       " 'onun',\n",
       " 'bli',\n",
       " 'panda',\n",
       " 'dost',\n",
       " 'gunta',\n",
       " 'anders',\n",
       " 'istles',\n",
       " 'zawsze',\n",
       " 'ol',\n",
       " '',\n",
       " 'memorized',\n",
       " 'insp',\n",
       " '',\n",
       " '',\n",
       " 'wurf',\n",
       " '',\n",
       " 'zk',\n",
       " '',\n",
       " '',\n",
       " 'kamera',\n",
       " '',\n",
       " '',\n",
       " 'conocer',\n",
       " '',\n",
       " 'issent',\n",
       " '',\n",
       " '',\n",
       " 'crianas',\n",
       " 'existe',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'maneira',\n",
       " 'temas',\n",
       " 'uego',\n",
       " '',\n",
       " 'Kampf',\n",
       " 'o',\n",
       " 'phants',\n",
       " 'gour',\n",
       " 'choreography',\n",
       " 'medios',\n",
       " '',\n",
       " '',\n",
       " 'ieben',\n",
       " 'sof',\n",
       " 'voltar',\n",
       " '',\n",
       " 'Picasso',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'rien',\n",
       " 'escuela',\n",
       " '',\n",
       " '',\n",
       " 'stanie',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'HJ',\n",
       " 'Adems',\n",
       " 'fas',\n",
       " '',\n",
       " 'drawers',\n",
       " 'semanas',\n",
       " 'beispielsweise',\n",
       " 'f',\n",
       " 'bild',\n",
       " 'onu',\n",
       " '',\n",
       " 'encias',\n",
       " 'i',\n",
       " 'gestellt',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'aurait',\n",
       " 'Nicki',\n",
       " '',\n",
       " '',\n",
       " 'a',\n",
       " 'avoir',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'conduction',\n",
       " '',\n",
       " 'zat',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Hola',\n",
       " '?',\n",
       " 'mellan',\n",
       " 'ktrej',\n",
       " 'cambio',\n",
       " 'vuel',\n",
       " 'clic',\n",
       " '',\n",
       " 'ncia',\n",
       " 'crucified',\n",
       " 'overst',\n",
       " '',\n",
       " 'tuck',\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tokenizer1.vocab.keys()) - set(tokenizer2.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20877"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokenizer2.vocab.keys()) - set(tokenizer1.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20878"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokenizer2.vocab.keys()) - set(tokenizer4.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51864"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer2.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29992"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer3.vocab['xi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
